{"pageProps":{"member":{"id":"syucream","nickname":"syucream","realName":"Ryo Okubo","bio":"A software Engineer","avatarSrc":"/avatars/syucream.jpg","sources":["https://syucream.hatenablog.jp/feed"],"githubUsername":"syucream","twitterUsername":"syu_cream","websiteUrl":"https://syucream.hatenablog.jp/"},"postItems":[{"title":"ユビーに転職して半年、慣れない技術スタックでも何とかやっていけている","contentSnippet":"みなさんいかがお過ごしでしょうか。 @syu_cream です。わたしは半年ほど前にユビー株式会社へソフトウェアエンジニア(以下 SWE)として転職しています。転職前後のストーリーは以下のエントリにて紹介しています。syucream.hatenablog.jp本記事では入社から半年後にかけての展開について綴ろうと思います。ユビーの SWE についてまず自分が在籍する、ユビーの SWE というポジションについて触れておきます。昨今のテック企業では選考の間に技術面接を経てポジションにマッチする技術力を持っているかを評価するケースが多々あると思われますが、ユビーではこのような技術面接は行っていません。その代わりに、特定の技術スタックへの理解が深いかどうかなどは問わず、プロダクト開発に欠かせない素養があるかを確認するため面接を設けています。この面接の体系についてはより詳しくまとめた記事があります。note.comユビーの SWE として入社した後の話入社時点でのスキル技術力を評価されずに入社した後、実際に業務を回せるのか。どのような活躍ができるのか。もしかしたら疑問に思われる方もいるかもしれません。さて、わたしもユビーの選考を受けた時にこちらの面接を受けています。そして実際、ユビーでは多用される Kotlin や GraphQL 、 React などの知識があるかは問われることなく選考を終えております。以下がユビーで採用している主要な技術なのですが、 React や Kotlin は少し触れた程度、 TypeScript や GraphQL は触れたことすらない程度で、実務レベルの経験をしているとはお世辞には言えない状態でした。（ GCP については幸いながら 3 年以上の経験がありました）入社後半年間のプロダクト開発業務そのような、技術スタックのマッチ具合でいうと芳しくない状態からここ半年間で、以下のような開発業務に携わっています。ユビーAI受診相談 と関連サービス ユビーリンク医療データを管理するマイクロサービス ( 参考 )Cloud Spanner の利用検討全体的に、必要な技術は必要になったその都度細かく学習しつつ、あまり学習に時間をさきすぎず開発を進めています。 React、 TypeScript の知識があまりないまま 1. のユビーAI受診相談の開発に着手した際は正直かなりの苦戦を強いられましたが、物怖じせず突っ込んでみて必要に迫られた知識を習得するのは自分の性分とは合っているように感じています。また GCP の利用経験があることから適度にインフラ周りのタスクも並行して取り、やろうとしていることが何も分からない！という状態を回避しています。そうして都度キャッチアップを繰り返していると既知の領域も広がってゆき、入社後半年経過した現時点では、分からないことも多々ありつつも開発を回せるようにはなってきています。そんな訳でスキルのミスマッチは何とかカバーすることはできているのですが、他方でタスクの見積もり精度が甘くなったり消化が遅れるなど、スキル不足ゆえにバリューを発揮しきれずもどかしさを感じるシーンもあります。ただこの点は現在は苦しみがあるものの、キャッチアップを続けていくうちに時間が解決してくれる部分もあるかと考えています。プロダクト開発以外での貢献ユビーの SWE はプロダクト開発以外にも OKR 作成と議論、ホラクラシー組織開発、採用などやるべきことが多々あります。それぞれ少なくないリソースを注ぐことになり、リソースが分散することに辛さもありつつも多岐に渡る活動ができることには面白さを感じています。この中で特にわたしがコミットしているのは採用です。例えば、その一環で直近ですと以下のようなユビーに興味を持ってくれる人が集まれる Slack ワークスペースの用意などを行っています（もしご興味がおありの方いらっしゃれば、ぜひご参加ください！！！）ユビーに興味があるエンジニアの人たちとユビーの中の人でもっとラフに非同期で話せる場所が欲しいな......と思ったので、このたびそれ専用の Slack ワークスペースを作ってみました！🥳ご興味がある方、以下フォームより参加表明を！何卒何卒〜〜！！✨✨✨https://t.co/QHChQPIuLK— しゅー　くりーむ (@syu_cream) 2021年12月16日  会社の様子というのは社外からは観測しがたく、またカジュアル面談は”カジュアル”とうたいつつも実施までに時間的・心理的ハードルが高い課題があるのではという仮説を持っています。これに対して、社内メンバーが同時に多数参加できかつ気軽なテキストベースのコミュニケーションが可能な Slack ワークスペースを設けることで一定対策できるのではと考えた次第です。すぐの採用に繋がらなくとも会社のプレゼンス向上に期待することも期待できます。採用ひとつとっても奥深く、プロダクト開発以外にもコミットすることで面白い経験ができています。おわりに以上、わたしがユビーに入社するあたりから半年目までの簡単な振り返りでした。技術スキルのマッチ具合やスキル不足で転職をためらう人は多々いると思いますが、わたしのように後からキャッチアップして辻褄を合わせるというやり方もあります。できないことが多い時期はそれなりの苦しみを感じますが、アンラーンして新しい技術を吸収できる素養さえあればやがて乗り越えられるはずですし、できることが増えた状態で見渡す景色は美しいものです。特にわたしが所属するユビーではアンラーンしてやっていける素養を重視するので、その点で不安を感じる方がいればご一考いただけると嬉しいです！最後に、 Meety 上でカジュアル面談募集しております。本記事やユビーについて気になったことがございましたら、ぜひご応募ください！meety.netmeety.netもっと気軽にお話したいご希望ありましたら、記事中で紹介した Slack ワークスペースもぜひ！ユビーに興味があるエンジニアの人たちとユビーの中の人でもっとラフに非同期で話せる場所が欲しいな......と思ったので、このたびそれ専用の Slack ワークスペースを作ってみました！🥳ご興味がある方、以下フォームより参加表明を！何卒何卒〜〜！！✨✨✨https://t.co/QHChQPIuLK— しゅー　くりーむ (@syu_cream) 2021年12月16日","link":"https://syucream.hatenablog.jp/entry/2022/01/31/154157","isoDate":"2022-01-31T06:41:57.000Z","dateMiliSeconds":1643611317000,"authorName":"syucream","authorId":"syucream"},{"title":"2021年Ubie Discoveryエンジニア入社エントリまとめてみた","contentSnippet":"はじめに今年も残り少ない日数となってきた今日この頃。みなさんにとって今年はどんな年になりましたか？ぼくの所属するユビー株式会社も今年も様々な飛躍と進化を遂げております。ubie.life特に多様なバックグラウンドを持つ優秀なメンバーが加入したことは目覚ましい点です。とりわけ「エンジニア」という枠 ( https://recruit.ubie.life/engineer 以下で定義されている JD ) で加入したメンバーは今年 20 人ほどいるようでした。とは言うもののさて、具体的にはどのようなメンバーがジョインしたのでしょうか。ユビーのメンバーは転職する際にいわゆる転職ブログエントリを書いているケースが多く、この場でいくつか抜粋して紹介させていただこうと思います！転職エントリを一部紹介！@itkq転職えんとりーです / Ubie 株式会社に入社していた https://t.co/anCRYSNhut— いたこ (@itkq) 2021年2月24日  まずは SRE の @itkq です。コロナ禍で長く続くリモートワークと事業への貢献を加味して転職への転換を振り返っています。@h13i32maruUbieという医療系ベンチャーに転職するので、ブログかいてみました。転職の意思決定（クックパッド → Ubie）https://t.co/LChrlFDghN— Ryo Maruyama (@h13i32maru) 2021年2月12日  次はソフトウェアエンジニアとして toB 向けサービス開発をしている @h13i32maru です。転職という意思決定をする上で、感情に流されず懸念要素を洗い出し整理していく良エントリを書いています。@empitsu88食べログからUbieに転職してそろそろ2ヶ月なので、いわゆる入社エントリを書きました。ソフトウェアエンジニアとしてUbieに入社しました #note https://t.co/vXyOFmqs2C— えんぴつ (@empitsu88) 2021年8月11日  お次もソフトウェアエンジニアの @empitsu88 です。働きやすさという観点も加えて転職を振り返ってます。@jagabassUbieに転職して1ヶ月が経ったので入社エントリを書きました！転職活動時の意思決定や入社後の感想を中心にお話ししています〜 https://t.co/2Zf2KLK5WP— じゃがじゃが (@jagabass) 2021年8月2日  @jagabass はデータアナリストとしてジョインしデータ分析スキルを武器に幅広く活躍しています。@syu_cream近況です。今後ともよろしくお願いいたします。https://t.co/8WpmGWqhe4— しゅー　くりーむ (@syu_cream) 2021年8月4日  筆者自身です。ソフトウェアエンジニアとして転職しています。コンフォートゾーンの脱出と自身のスキルセットの多様化などの思惑を踏まえ、ユビーへのジョインを決断しています。@dtaniwakiUbieに入社して1ヶ月経ったので、入社エントリーを書きました。#Ubie #ユビー #SRE #Kuberneteshttps://t.co/q5p9CHiNXY— Daisuke Taniwaki (@dtaniwaki) 2021年9月7日  SRE として @dtaniwaki ジョインして、持ち前の技術力の高さを生かしてインフラ整備やイベント登壇など多岐に渡る活躍をしています。@m_mizutani書きました。9月からはUbieという医療系のスタートアップで引き続きセキュリティエンジニアをやっていきます。各位何卒よろしくお願いいたします。転職します（Cookpad → Ubie） - ある研究者の手記 https://t.co/KyA6SA8EHL— mizutani (@m_mizutani) 2021年8月1日  @m_mizutani は比較的最近できた セキュリティエンジア ポジションでジョインしています。最近は一人アドベントカレンダーを回すなどしており...正気か！！？adventar.org@shikajiroZOZOテクノロジーズ/ZOZO研究所を退職しました。次は医療xAIスタートアップUbieに行ってきます。｜shikajiro @shikajiro #note https://t.co/jsRGXr2red— しかじろう (@shikajiro) 2021年8月5日  @shikajiro はソフトウェアエンジニアとして入社しており、現在 toB 向けサービスの開発に従事しています。「ML以外のエンジニアリング全部やる」をモットーに活躍してきたところからの転職、エモさ溢れるエントリを書かれています。@hokaccha近況です | クックパッドを退職して Ubie Discovery に転職しました - hokaccha memo https://t.co/N3KdNGIacH— hokaccha (@hokaccha) 2021年9月1日  @hokaccha もソフトウェアエンジニアとして入社しております。彼もまたエモさ溢れるエントリをしたためています。とりもどそう、青春。@okiyuki99本日12/1からUbieに入社しました。スタートアップらしく、崖の上から飛び降りながら飛行機を作る環境をEnjoyしていきたいと思います— OKIYUKI (@okiyuki99) 2021年11月30日  ブログエントリではないですが、データアナリストとして @okiyuki99 もジョインしています。早速多角的にデータ分析をしてプロダクトにバリバリ貢献しています。ohtaman本日から Ubie Discovery です！BrainPadのみなさん、12年間ありがとうございました！ #はてなブログ12年お世話になった BrainPad から Ubie へ行きます - 0-8 https://t.co/FpmJkO0Kyg— おおたまん (@ohtaman) 2021年11月30日  ohtaman は前職において CDTO （最高データ技術責任者）という立場におり、ユビーには機械学習エンジニアとしてジョインしています。 12 年間にわたる前職の振り返りとユビーでの展望、読み応えがあります。@takahi_i書きました！ /  Ubie Discovery に転職しました（人生初の入社エントリ）｜takahi_i @takahi_i #note https://t.co/BWBn79tc5s— Takahiko Ito (@takahi_i) 2021年9月17日  @takahi_i は機械学習や検索を得意とするエンジニアであり、「MLがサービスの根幹で使われている」ことなど専門性を活かせる職場であることをエントリで振り返っています。おわりに上記に挙げたのは一例で、他にも紹介しきれない魅力的なメンバーがおります。優秀なメンバーが揃うドリームチームが出来上がりつつあり、個人的には良いメンバーと一緒に働けるこの環境がある種の福利厚生にも思えてきています。が、ユビーの野望は果てしなくチャレンジしたい領域が多分にある一方まだまだ人が足りません。まだまだ手の届いていない課題が山ほどあります！このエントリをみているあなたの将来の選択肢にユビーを考えてみませんか？ユビーについてもっと知るには、以下のページをご覧頂けると幸いです。recruit.ubie.lifeMeety をやっているメンバーも多く、カジュアル面談もお気軽にご参加いただけます。meety.netまた、最近ユビー在籍エンジニアとラフに話せる専用 Slack ワークスペースを用意してみました。招待制になっておりフォームの入力が必要になってしまいますが、ぜひお気軽にご参加ください。カジュアル面談より更にカジュアルな非同期に話を聞く場として、あるいはユビーの雰囲気を感じ取ることを目的として、ご活用いただけると嬉しいです！ユビーに興味があるエンジニアの人たちとユビーの中の人でもっとラフに非同期で話せる場所が欲しいな......と思ったので、このたびそれ専用の Slack ワークスペースを作ってみました！🥳ご興味がある方、以下フォームより参加表明を！何卒何卒〜〜！！✨✨✨https://t.co/QHChQPIuLK— しゅー　くりーむ (@syu_cream) 2021年12月16日","link":"https://syucream.hatenablog.jp/entry/2021/12/20/170222","isoDate":"2021-12-20T08:02:22.000Z","dateMiliSeconds":1639987342000,"authorName":"syucream","authorId":"syucream"},{"title":"転職しました: メルペイ -> Ubie ~クセつよ組織を求めて~","contentSnippet":"表題の通りで、 2021年7月より前職の株式会社メルペイ（メルカリ）を退職して Ubie に転職しました。というわけでいわゆる転職エントリです。TL;DRメルカリ・メルペイに 3 年 10 ヶ月在籍しましたスキルや経験の幅を持たせたい、サービス初期の雰囲気をまた味わいたくて転職Ubie に入社して SWE として働き出して一ヶ月経過しましたメルカリ・メルペイでやってたことメルカリには 2017 年 8 月に入社し、約半年間 SRE チームに所属していました。その後メルペイの立ち上げに合わせて異動して 2021 年 6 月末までデータエンジニアをやっていました。本当に数多くの同僚や関係者に恵まれておりました。お世話になった方々ありがとうございました。メルカリ・メルペイで体験したことは枚挙に暇がなく、約 4 年で体験したことなのかと疑うほどに濃密なものでした。プロ集団の SRE チームの一員として働き、刺激を受けたこと。メルペイの立ち上げ初期メンバーに加わり、大規模な新サービスのローンチに関われたこと。マザーズ上場。社員数の急拡大（自分が入社した頃より最終的に約 3 倍になった！）と多様なバックグランドを持つメンバーが揃い出したこと。大量のデータを取り扱うデータ基盤の構築にリソースを振り切れたこと。特にデータ基盤の構築は約 4 年の歳月の中で注力していた事であり、成果を Builerscon 2018 LT や ApacheCon NA 2019 で発表させて頂くにも至りました。 speakerdeck.com speakerdeck.comメルカリ・メルペイは先日発表があったメルカリShops や、メルコインも手がけており、現状に満足せずに Go Bold に挑戦し続る、刺激の強い環境です。about.mercari.com将来に向けた葛藤そんな良い環境を手放してまで転職する理由としては、ひとえに自分のキャリアと将来を考えた上で別の道を歩むのが良いと判断したためです。転職に至るまで以下のような葛藤を抱えていました。自分の年齢と人生を考慮して今もっと挑戦すべきと感じた自分の立場がコンフォートゾーンになってきたスタートアップの雰囲気をより感じたくなったそれぞれ分解してみます。自分の年齢と人生を考慮して今もっと挑戦すべきと感じた今年で自分は 33 歳であり、今後のライフイベントも考慮するとどこまで自由に挑戦できるか疑問が浮かびます。今このとき、体力や情熱があり多少人生の時間の使い方を自由にできる時により挑戦すべきではないかと感じていました。自分の立場がコンフォートゾーンになってきたありふれた話ですが、自分の立場がコンフォートゾーンになり、新しい刺激が少なく成長をあまり実感できなくなっていました。メルカリ・メルペイでは主にデータ基盤の開発と運用をやっていました。こうしたいわゆる社内プラットフォームやプロダクト基盤などと呼ばれるシステムの担当分野においては、特定の技術的な問題と挑戦にフォーカスしやすい反面、ビジネス上のドメイン知識の獲得や多様な技術の理解が得難いというデメリットがあると考えます。また基盤刷新や新技術の導入で新しい知識を獲得する機会はあるものの、徐々に強くてニューゲームになり刺激が無くなってくるようにも感じます。自分の直近の実例を取ると、 Apache Airflow や Apache Beam, Apache Spark, Apache Flink を活用したデータ収集基盤の構築や BigQuery による出たウェアハウス提供に従事してきました。他方、連携するマイクロサービスの知識はそれほど身に付かず、 Android, iOS アプリや Web のフロントエンドについては更に距離のある話題になっていました。前前職まで遡ると、自分のキャリアにおいてプラットフォーム的な役割に振った時間は 8 割ほどを占めています。この方向性を維持するにも別の道を模索するにも、より多様な経験を積んで能力や発想に幅を持たせるべきではないかと考えました。スタートアップの雰囲気をより感じたくなった自分がメルカリに入社した頃と比べて社員数は劇的に変化しました。組織は成熟して上場も果たし、仕事の仕方も自分の入社した頃と比較してこなれてきた印象がありました。それに際してどこか「昔はよかった」と感じてしまうシーンが増えてきました。（大抵は思い出を過剰に美化しているかもですが）考えるべきこと、やるべき仕事が多量にある急成長中のスタートアップでは刺激も多く学べることがあるはずです。自分には今、再びそのカオスだけど喜びが多い環境に身をおくべきなのではと葛藤しました。Ubie 入社前述のような葛藤をかれこれ半年ほど続けて、最終的に 2021 年 7 月に医療系スタートアップである Ubie 株式会社に入社する運びになりました。ubie.lifeUbie は現在ホットな医療という分野での活躍をねらい、また特色のあるというかクセが強い組織や文化を持っており、これが自分が抱える課題を緩和する材料になると感じたためです。医療 X テクノロジーは挑戦しがいがある代表の阿部の記事の通り、医療分野においてテクノロジーを組み合わせて解決できる課題があり、挑戦しがいがあります。note.com「テクノロジーで人々を適切な医療に案内する」ことで救える命、助けられる人々が居るはずです。もっと身近に言うなら将来の自分を救うことにもなりえます。俺もまだまだ若いし放っておけば直るっしょ、という油断をいつまでできるか分かったものではありません。このコロナ禍で、我々は誰しも当事者になりうるという危機感を植え付けられています。医療と将来の健康状態は今改めて我々の目の前に見える壁として立ちはだかっています。そして課題があるということは、ビジネスチャンスにも満ちていると言えるでしょう。会社の成長に貢献して、対価としての報酬を得る。自分が事業にコミットしている実感が非常に出る状況が揃っていると考えます。ホラクラシー組織と頻繁な異動Ubie ではホラクラシーという組織体系を採っています。詳細は以下にまとまっています。個人的に響いた点として、有機的に組織が変化して異動が頻繁に行われていること、個人が複数のロールに紐付き業務を遂行することが挙げられます。（実際、入社して二日目で自分のメンターが異動してチームから居なくなりました。ワロタ、そんなのってアリ！？）note.comロールベースの働き方は、現在多様な経験を積んで引き出しを増やしたく感じている自分にとってマッチしそうです。 will/can/must がマッチしそうな環境に身を置き、別の道があれば別のロールを探すこともできそうです。これだけ言うと兼務に近いイメージになりますが、 Ubie のホラクラシーではロールを積極的に抜けていく想定でもあり、機動力高く動いていけそうです。さて一風変わったそんな組織にジョインして一ヶ月。現在は「生活者とかかりつけ医をつなぐ」為のプロダクト開発のため、ソフトウェアエンジニアとして動き初めています。先日新サービスに関するプレスも打たれました。prtimes.jp関わる技術スタックとしてはフロントエンドで React(Next.js), TypeScript バックエンドでは Kotlin, Spring Boot, GraphQL などで、正直ほとんど触ったことない（特に TypeScript は入社まで触ったことが全くなかった）要素ばかりです。しかしこれもまた良い刺激でありコンフォートゾーンからの脱出が捗ると考えています。まだまだバリバリのスタートアップである自分が入社した時点で社員数は 100 名超くらいでした。その中でも Web 周りを触ってるソフトウェアエンジニアに限定すると 15 名ほどと少数精鋭部隊になっています。Ubie もエンジニアが少しずつ増えてきたんですが、SWE に絞ると今だに15人くらいしかいない状況です...世界に羽ばたくヘルスケアプロダクトを作りたい方絶賛募集しているので応募してください！https://t.co/gezcIyiibz— 久保 恒太 / Ubie CEO (@quvo_ubie) 2021年7月6日  twitter.comということは個々人やれることややるべきことは無限にあるわけです。成長できる要素しかないし、このフェーズの雰囲気を味わえることは非常に価値があります。また会社の規模と事業のフェーズも手伝ってか率直かつ建設的なコミュニケーションが行われています。根回しやプライベートなチャットなど社内政治を避け、かつ同僚にラフにコミュニケーションを取る体制は、自分には快く感じるに加えてバリバリの成長途中感を覚えさせるものになっています。おわりにというわけで、メルカリ・メルペイに 3 年 10 ヶ月在籍しましたスキルや経験の幅を持たせたいサービス初期の雰囲気をまた味わいたくて転職Ubie に入社して SWE として働き出して一ヶ月経過しましたというお話でした。今後ともよろしくお願いいたします。また本記事がなにかの参考になれば幸いです。もし本記事を通して Ubie にご興味を持っていただけた方がいらっしゃいましたら、採用ページあるいは僕に直接ご連絡いただけると幸いです！あんまり興味を持った訳じゃなくて最近どうなの？って話でも振って頂けると喜びます。先述の通り Ubie ではまだまだメンバーを募集しており、更にプロダクト開発を邁進していきたい状況です。SRE やデータエンジニアについては固有の JD があり、recruit.ubie.liferecruit.ubie.lifeWeb 周り触ってるソフトウェアエンジニア（僕の今の本業はここ）についてはこちらになります。recruit.ubie.life","link":"https://syucream.hatenablog.jp/entry/2021/08/04/125435","isoDate":"2021-08-04T03:54:35.000Z","dateMiliSeconds":1628049275000,"authorName":"syucream","authorId":"syucream"},{"title":"技術書典9で「Apache Parquet ではじめる快適 データ分析」を出します","contentSnippet":"技術書典9 で「Apache Parquet ではじめる快適 データ分析」を出します。もしよろしければお手にとっていただければ幸いです。まあ今回はオンライン開催で電子書籍のみの配布なので、物理的にお手に取れないんですけどね〜！本書は Apache Parquet についてつらつらと紹介記事を書いた内容になります。また付録的に同サークルメンバー著「USB デバイスを作るのがツラい」というテーマの記事も掲載します。データ分析業務にムッチャ関わる、ストレージコストを最適化したい、 BigQuery などのデータウェアハウスサービスを日常的につかう、なんとなく気になった、ような人々に効果的だと思います。以上よろしくお願いいたします。目次:第1章 Apache Parquet ではじめる快適データ分析 51.1 はじめに .................................. 5 レコード指向フォーマットとは? ..................... 6 カラムナフォーマットとは? ....................... 8 レコード指向とカラムナ、OLTPとOLAP ............... 10 カラムナフォーマットの実装例 ...................... 111.2 ApacheParquetとはなにか........................ 11 並列読み書き処理化しやすいバイナリレイアウト............. 12 スキーマが自己記述的 ........................... 15 シンプルで柔軟性のある型表現 ...................... 15 ネストされたカラムや繰り返しされるカラムに対しても有効 . . . . . . . 16 多様なエンコード方法 ........................... 26 豊富な圧縮コーデックを選択可能 ..................... 31 メタデータを駆使したクエリ最適化が可能 ................ 321.3 ApacheParquet実装例 .......................... 33 parquet-mr................................. 33 ApacheArrowC++実装......................... 34 Goにおける実装例............................. 341.4 実際に使ってみる ............................. 35 Parquetファイルを生成してみる ..................... 35 ParquetファイルにAthenaからクエリしてみる . . . . . . . . . . . . 391.5 実際の運用................................. 42 Parquet で実際どれくらいファイルサイズが削減されるのか? . . . . . 42 RowGroupとPageのサイズのチューニング .............. 42 長期ログ保存におけるコスト削減に寄与できる .............. 42 ストリーム処理に組み込む難しさを考慮する ............... 43 SELECT*に弱い............................. 44 1.6 おわりに .................................. 45付録 AA.1 はじめに .................................. 46A.2 USB通信プロトコル概要 ......................... 46A.3 USBデバイスの設計方針 ......................... 48A.4 USBデバイスが動くまで ......................... 49A.5 USBの消費電力規格............................ 50A.6 地獄のノイズ耐性試験 ........................... 50トグルビット不一致 ............................ 51安物のハブが...... ............................. 52A.7 まとめ ................................... 52あとがき54USB デバイスを作るのがツラい 46@syu_cream .................................... 54 @lunatic_star ................................... 54","link":"https://syucream.hatenablog.jp/entry/2020/09/08/225620","isoDate":"2020-09-08T13:56:20.000Z","dateMiliSeconds":1599573380000,"authorName":"syucream","authorId":"syucream"},{"title":"Data Mesh の記事を読んだ","contentSnippet":"一年以上前の記事だけど、 https://martinfowler.com/ に \"Data Mesh\" をうたう記事があったので軽く読みました。martinfowler.comこちらに日本語で概要をまとめた記事もありご一読することをおすすめします。僕の個人ブログを見るより確実で良い情報を得られるでしょう。https://www.infoq.com/jp/news/2020/03/distributed-data-mesh/以下では現行のぼくの業務と照らし合わせて、 Data Mesh について個人的解釈などを書いていきます。Current status ...二年くらい前に builderscon で \"メルペイにおける、マイクロサービスに寄り添うログ収集基盤\" みたいなタイトルで LT で発表したりしました。当時、急速に開発されるマイクロサービス群と元から存在したモノリスなシステムに特化したデータ基盤が存在し、「マイクロサービス化したら分析等のためのデータどうなんの？？？」と漠然とした課題感はあるものの誰も答えを見出だせていない状況でした。 speakerdeck.comそこから二年も経過すると弊データ基盤も色々とあり、上記に挙げた batch/streaming それぞれの要件に特化した仕組みを作ったり刷新したり、公開していないまた別のシステムを構築したりとかしていました。（その辺の最近の話も別途公開していければと思っています）この二年で発生した大きな変化としては、以下の辺りが挙げられるかと思いますビジネスのスケールに対して自分が認知できる範囲が追いつかなくなったマイクロサービスがむっちゃ増えた。俺は数えるのをやめたデータの要件も多種多様になった。種類によるところや性能要件などこの辺りの煽りを受けると、データ基盤もこのような変化に追従できなければ組織の中でのボトルネックになりかねないなという危機感を覚えています。Data Mesh の話という個人的振り返りをしつつ元記事の話題に移ります。データ基盤前史とにかく我々は「サイロ化」という言葉を好んで使い、打ち倒すべき敵みたいに扱います。データのサイロ化もそのやり玉に挙がり、組織やシステム間でデータ連携ができずに分析基盤でうまく扱えない課題を指摘されることがあります。これに対して、データレイクやデータウェアハウスみたいな一元的にデータが管理可能な入れ物を用意して、とりあえずそこにデータを突っ込む道路を舗装して分析業務を回すみたいな解が取られてきたと思います。Data Mesh の記事ではこのような一元的なアーキテクチャを前世代的なものと位置付けています。中央集権的なデータ基盤は全体最適化には良いけれど、個別の高度な要件を満たすのが難しくなります。またデータ基盤はデータの producer / consumer のようなデータの流れに沿った上流・下流の構図を作りがちです。んで、 consumer が要件を満たしたい場合上流に遡りつつデータ基盤屋さんにも相談するような依存関係が生まれます。さらにそうした構図が生まれると中流に位置するデータ基盤のチームは時として producer/consumer のドメイン知識を求められるかもしれません。その振る舞いを行えるメンバーがどれだけ確保できるでしょうか・・・。個人的にはこうしたデータ基盤のモノリス化はなんら不思議ではないと思います。BigQuery はじめとした便利なデータ基盤に使えるシステムが台頭してきてはいますが、データエンジニアリングの領域は未だ職人芸が求められる領域であり、それに特化したスペシャリストが基盤構築を行うのは自然かなと。またデータ基盤構築にあたり、まずデータを一定数揃えないとバリューを出しにくいでしょうから producer に寄った最適化をして「とりあえずデータを集める」「データレイクに突っ込んでから後のことを考える」のは理にかなっていると考えます。とはいえデータ基盤の利用者が増えて、 consumer のリクエストを聞き始めると苦しみが生まれ始めるとも考えられます。自分の実体験としても、黙っててもデータ基盤がワークするケースというのは producer と consumer が同一のチームかあまり距離が遠くないチームのケースが多いような気もしています。データをメッシュにするこの記事における前世代的なデータ基盤の課題の解決方法は、マイクロサービスアーキテクチャさながらモノリスの分解だと考えられます。データメッシュの世界では一元的でモノリスなデータ基盤は存在せず、代わりに広く使われるデータインフラを見るチームと分散したデータ処理システムが存在します。また明確な producer と consumer という立場を生じさせず、各ドメインチームがデータの管理も行い相互にコミュニケーションします。分散することで前述のサイロ化問題が再熱しそうですが、横断的なデータガバナンスの仕組みやセルフサーブ可能なエコシステムを導入していきます。データメッシュの思想は本質的には権限や責務の移譲と、データ基盤が真に基盤らしく振る舞うためのパラダイムシフトを起こすことだと考えます。前者の思想はマイクロサービスアーキテクチャとよくなじみ、データの producer がマイクロサービスであるならばその延長でデータも扱えればいいだけでしょう。データ基盤が基盤本来の仕事に集中するのも重要なことで、データの producer / consumer が増えるにつれ無限にドメイン知識が求められるなら組織のスケーラビリティは死んでいくし、同様の振る舞いができるメンバーを探すのが困難になってくると思います。ぼくの所属する組織では Microservices Platform チームというマイクロサービスを支える基盤を構築するチームが存在し、マイクロサービスを開発運用するにあたり共通課題となる Kubernetes クラスタやデプロイパイプラインの提供を行っています。これに近く各ドメインチームがデータにまつわる課題を解くための共通基盤を提供してセルフサーブ可能にして、しかし自身は課題を解く主役にはならないぐらいのバランスが求められるのかもしれません。tech.mercari.comそう理想は言ってもデータメッシュの世界観に沿うようなツールが無いとこの理想的世界に近づくことはかなわないでしょう。データメッシュの記事では特に GCP のプロダクトについて、一元的なデータガバナンスなら Google Cloud DataCatalog が、バッチ・ストリーミング処理には統合的に扱えインフラがフルマネージドな Google Cloud Dataflow があると挙げています。また筆者の経験ではデータメッシュの世界観でデータレイク的なポジションとして GCS を、ドメインごとに bucket を作成して利用して、データウェアハウスとして BigQuery を使うのもありかと考えます。特に BigQuery は GCP プロジェクトが異なっていても参照する権限があれば JOIN することは可能であり、データメッシュのような論理的には分散したデータ基盤を実現するのにマッチするように感じます。Data Mesh と俺セルフサーブ可能な基盤を目指してなるべくデータ基盤がドメイン知識を抱え込まずコミュニケーションにおけるクリティカルパスにならないようにする思想は重要だと感じます。前世代的な（と言われてしまった）データ基盤では producer/consumer のバリエーションも増えて、その間のコミュニケーションにデータ基盤が入ることでボトルネックを生むことになりかねません。セルフサーブ可能であればある程度「勝手にやってくれ」といえる領域が増えてボトルネックが解消されてゆき、データ基盤チームはより基盤の作り込みに集中することができると思われます。とはいえこれを最初期からゴールに据えるのも骨が折れる作業であると思うので、段階的に分散可能にしていくのが良いかもしれません。最近では弊チームでもセルフサーブ・分散管理可能な設計にしつつ、枯れてくるまでは自チームで面倒を見るという思想で動くことが増えてきました。データガバナンスやデータ処理の分散化そのものについてはやや懐疑的な部分があります。前世代のデータ基盤でも十分多い数の producer が発生するはずで、データメッシュの話とは独立してデータガバナンス、メタデータ管理やリネージ追跡、クオリティチェックなどの課題を考えるべきでしょう。もしかしたらデータ基盤チームがこれらの課題まで人手でカバーしているケースがあるかも知れませんが、それならなおのことデータメッシュの文脈に依らずエコシステムの作り込みをした方が良いように思えます。またデータ処理もまた職人芸が試される領域でありあまり各ドメインチームに移譲しにくいような気もしています。BigQuery などデータウェアハウスに格納してから SQL でなんとかする、みたいな汎用的なシナリオならいざ知らず、低遅延での処理が求められるとか重複除去したいとかリッチな要件が出てくるシナリオで各チームで対応するのが現実的なのかどうか。また、いずれにせよ consumer のようなデータを使う側にある人々をどのようにケアするかは課題になると推測しています。中央集権的なデータ基盤の有無に関わらず consumer が必要なデータを producer に準備してもらう枠組みは必要で、そのコミュニケーションや動機づけをどうすれば解決できるのか自分の中ではアイデアがありません。そこを含めてデータガバナンスで頑張る！という話であるなら、まだ現実の課題に適用するまでに障壁がある気もしております。","link":"https://syucream.hatenablog.jp/entry/2020/07/07/000212","isoDate":"2020-07-06T15:02:12.000Z","dateMiliSeconds":1594047732000,"authorName":"syucream","authorId":"syucream"},{"title":"Avro と BigQuery の load とうまく付き合いたい","contentSnippet":"Avro と BigQuery の読み込みApache Avro は BigQuery のデータ読み込みに対応したシリアライゼーションフォーマットであり、 Object Container Files フォーマットを採用することでスキーマが自己記述的になり読み込みに際して別途スキーマ情報を与えなくて済むメリットがあります。また BigQuery としては Avro (を含めたいくつかの形式) では平行読み込みが可能とされ、それができない形式、たとえば gzip 圧縮された JSONL 形式などと比較して早く読み込めるようです。cloud.google.com加えて、実は並行読み込みが可能とされてかつ効率的な圧縮が期待できる Avro, Parquet, ORC の中にも読み込み処理において優劣があるようです。Google BigQuery: The Definitive Guide によると \"The most efficient expressive format is Avro\" とあり、列志向で圧縮もかかる Avro が最も効率的であるとされています。対して Parquet や ORC は行志向であり、これはこれで外部デーブル経由でファイルに直接クエリする分には効率的なものの、 BigQuery に読み込む際には全列読まなければならない分 Avro が有利なようです。そんないい感じっぽい Avro の BigQuery へのデータ読み込み、本記事ではスキーマ周りについていくつか動作を確認してみようと思います。BigQuery にさまざまなスキーマの Avro ファイルを読ませてみる互換があるスキーマで追記する場合まずは replace をせず単純に BigQuery のテーブルにレコードを追加して行こうと思います。とりあえず適当に 1 カラムだけある空テーブルを作っておきます。まずはこれにマッチする単純な Avro ファイルを作って load してみます。$ cat user_v1.avsc{  \"name\": \"User\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    }  ]}$ java -jar ~/tools/avro-tools-1.8.2.jar random --schema-file user_v1.avsc --count 1 user_v1.avro 2>/dev/null$ bq load --project_id syucream-dev --source_format AVRO syucream-dev:test_syucream.user user_v1.avroUpload complete.Waiting on bqjob_r750d0087bb28a293_00000172515bed9d_1 ... (1s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r7bab3c778b4c6a7d_00000172515c7520_1 ... (0s) Current status: DONE+---------------------+|         id          |+---------------------+| 7190660540979993749 |+---------------------+サクッとできました。追記もサクッとできます。$ bq load --project_id syucream-dev --source_format AVRO syucream-dev:test_syucream.user user_v1.avroUpload complete.Waiting on bqjob_r26360ddf0526570_00000172515d711c_1 ... (1s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_rba99937746b1712_00000172515d8f6f_1 ... (0s) Current status: DONE+---------------------+|         id          |+---------------------+| 7190660540979993749 || 7190660540979993749 |+---------------------+このスキーマと互換のあるスキーマを持つ Avro ファイルの load も問題なくできます。$ cat user_v1_1.avsc{  \"name\": \"User\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    },    {      \"name\": \"name\",      \"type\": [\"null\", \"string\"],      \"default\": null    }  ]}$ java -jar ~/tools/avro-tools-1.8.2.jar random --schema-file user_v1_1.avsc --count 1 user_v1_1.avro 2>/dev/null$ bq load --project_id syucream-dev --source_format AVRO --schema_update_option ALLOW_FIELD_ADDITION syucream-dev:test_syucream.user user_v1_1.avroUpload complete.Waiting on bqjob_r616f9c494daf502a_00000172516056b9_1 ... (0s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r46005fd5ddeefbab_00000172516077d6_1 ... (0s) Current status: DONE+----------------------+------+|          id          | name |+----------------------+------+| -6779445778023123159 | NULL ||  7190660540979993749 | NULL ||  7190660540979993749 | NULL |+----------------------+------+BigQuery のスキーマ的には互換がある追記をする場合今度は前述とは互換性がない、 name フィールドが nullable でなくなったスキーマを持つ Avro ファイルを load してみます。これは成功しますが BigQuery のテーブルとしては name フィールドは nullable のままとなります。（まあ nullable から required の変更は許されていないですしね。。。）$ cat user_v2.avsc{  \"name\": \"User\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    },    {      \"name\": \"name\",      \"type\": \"string\"    }  ]}$ java -jar ~/tools/avro-tools-1.8.2.jar random --schema-file user_v2.avsc --count 1 user_v2.avro 2>/dev/null$ bq load --project_id syucream-dev --source_format AVRO --schema_update_option ALLOW_FIELD_ADDITION syucream-dev:test_syucream.user user_v2.avroUpload complete.Waiting on bqjob_r5a6eb8678879a075_0000017251620b01_1 ... (1s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r5105091904b8eed0_000001725162d619_1 ... (0s) Current status: DONE+----------------------+-----------------+|          id          |      name       |+----------------------+-----------------+|  3285309633976168209 | twbnsureievqwes ||  7190660540979993749 | NULL            || -6779445778023123159 | NULL            ||  7190660540979993749 | NULL            |+----------------------+-----------------+ここから逆行して最初に load した Avro ファイルを load しようとしても成功します。これはやはり BigQuery のテーブル上では name フィールドは nullable であり、 name フィールドをそもそも持たないレコードの場合は null で埋めればいいからですね。$ bq load --project_id syucream-dev --source_format AVRO syucream-dev:test_syucream.user user_v1.avroUpload complete.Waiting on bqjob_r3e8bbde894d35c3b_00000172516328ff_1 ... (0s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r7e87a10bad564007_00000172516369af_1 ... (0s) Current status: DONE+----------------------+-----------------+|          id          |      name       |+----------------------+-----------------+|  7190660540979993749 | NULL            ||  3285309633976168209 | twbnsureievqwes ||  7190660540979993749 | NULL            || -6779445778023123159 | NULL            ||  7190660540979993749 | NULL            |+----------------------+-----------------+互換がない追記をする場合今度はさらに BigQuery のテーブルとしても互換が取れないであろう変更をしてみます。ここでは required となる age フィールドを追加してみます。この場合、この Avro ファイル単体としては load できそうですが既存のレコードが age フィールドの値をもたないため load できません。$ cat user_v3.avsc{  \"name\": \"User\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    },    {      \"name\": \"name\",      \"type\": \"string\"    },    {      \"name\": \"age\",      \"type\": \"long\"    }  ]}$ java -jar ~/tools/avro-tools-1.8.2.jar random --schema-file user_v3.avsc --count 1 user_v3.avro 2>/dev/null$ bq load --project_id syucream-dev --source_format AVRO --schema_update_option ALLOW_FIELD_ADDITION syucream-dev:test_syucream.user user_v3.avroUpload complete.Waiting on bqjob_r6d0dccd4050d198_000001725165f2c9_1 ... (0s) Current status: DONEBigQuery error in load operation: Error processing job 'syucream-dev:bqjob_r6d0dccd4050d198_000001725165f2c9_1': Provided Schema does not match Tablesyucream-dev:test_syucream.user. Cannot add required fields to an existing schema. (field: age)--replace する場合BigQuery のデータ読み込みでは追記ではなくアトミックなテーブルの再生成も行えます。この場合はテーブルとそのスキーマが作り直される都合、前述のスキーマの互換性を気にしなくてよくなります。$ bq load --project_id syucream-dev --replace --source_format AVRO syucream-dev:test_syucream.user user_v1.avroUpload complete.Waiting on bqjob_r1441ed0e2b659303_000001725167e0af_1 ... (0s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r64a104d745d627a1_00000172516817e3_1 ... (0s) Current status: DONE+---------------------+|         id          |+---------------------+| 7190660540979993749 |+---------------------+$ bq load --project_id syucream-dev --replace --source_format AVRO syucream-dev:test_syucream.user user_v1_1.avroUpload complete.Waiting on bqjob_r1254a5673d1c4a55_0000017251684053_1 ... (0s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r542e92fd2eadaefe_0000017251685b41_1 ... (0s) Current status: DONE+----------------------+------+|          id          | name |+----------------------+------+| -6779445778023123159 | NULL |+----------------------+------+$ bq load --project_id syucream-dev --replace --source_format AVRO syucream-dev:test_syucream.user user_v2.avroUpload complete.Waiting on bqjob_r7f71a4c5acbb4a54_00000172516874f8_1 ... (0s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r26844a99b599657a_0000017251688ed0_1 ... (0s) Current status: DONE+---------------------+-----------------+|         id          |      name       |+---------------------+-----------------+| 3285309633976168209 | twbnsureievqwes |+---------------------+-----------------+$ bq load --project_id syucream-dev --replace --source_format AVRO syucream-dev:test_syucream.user user_v3.avroUpload complete.Waiting on bqjob_r3039d309e0eaecd0_000001725168a3bd_1 ... (1s) Current status: DONE$ bq query --nouse_legacy_sql 'SELECT * FROM syucream-dev.test_syucream.user'Waiting on bqjob_r3535c0ed97eae63f_000001725168c9b7_1 ... (0s) Current status: DONE+---------------------+-----------+----------------------+|         id          |   name    |         age          |+---------------------+-----------+----------------------+| 6571829868147110661 | gcypqmwby | -7543339857203188581 |+---------------------+-----------+----------------------+雑なまとめBigQuery による Avro ファイルのデータ読み込みは非常に協力で、 --replace によるテーブル更新によって楽で効果的な運用ができると思います。BigQuery 上でスキーマがどうなるかを考えずに読み込みジョブを実行するだけで良くなるのはメリットが大きいでしょう。ただし --replace ですべてのユースケースが叶えられるわけでもなく、巨大なデータソースから ETL を経て差分更新で同期したいだとかログをひたすら追記したい場合にはスキーマの更新問題がしばしばネックになると思われます。ただその場合でも、BigQuery のテーブルのスキーマとして互換が取れる Avro ファイルであれば読み込み可能であることからスキーマ更新について考える負荷は減りそうです。スキーマ更新が頻繁に発生しうるワークロードでは逆にこの特性を捉えた上でどうテーブル更新するかのワークフローを組むと良いのかもですね。","link":"https://syucream.hatenablog.jp/entry/2020/05/27/000215","isoDate":"2020-05-26T15:02:15.000Z","dateMiliSeconds":1590505335000,"authorName":"syucream","authorId":"syucream"},{"title":"Cloud Dataflow の FlexTemplate は何者か","contentSnippet":"先月、さらりと Cloud Dataflow に FlexTemplate という新機能のベータ版がリリースされました。cloud.google.com残念ながらまだあまりドキュメントがなく、これを用いるとなにが嬉しいのかが掴みにくいところです。本記事では FlexTemplate 周りを軽く試してどのような機能で何が嬉しいのか探ってゆきます。従来のテンプレートCloud Dataflow はジョブの一部パラメータを実行時に置き換え可能にしたビルド済みのテンプレートを作ることができます。また Google はこの機能を用いた公式テンプレートを何種類か用意してくれており、 GCP 上のリソースに対してコーディングなく ETL 処理をすることが可能になっています。cloud.google.comちなみにこのテンプレートの機能なのですが、 FlexTemplate の登場の都合からかドキュメント上で \"従来のテンプレート\" という見出しになってしまったようです。。。実行時にパラメータを置き換え可能にするには、その値を ValueProvider でラップしてあげる必要があります。ValueProvider の実装としてテンプレートのステージング時に静的に値が決まる StaticValueProvider と実行時に決まる RuntimeValueProvider が存在しており、これによりパラメータを渡すタイミングを柔軟に選べます。また Apache Beam の SDK 内の IO 関連ビルダークラスには ValueProvider を受け取って振る舞いを変えてくれるものが多々あります。beam.apache.org逆に言うとこの ValueProvider で賄えないような性質のパラメータは実行時に指定可能なパラメータとして扱えません。これは分かりやすい部分で言うと Beam の SDK のインタフェース的に ValueProvider を受け取ってくれないような部分、もっと複雑な例で言うと内容によってパイプラインのグラフが変わるようなパラメータは扱えません（あるいは扱うのが困難）FlexTemplateFlexTemplate は、おそらくなのですが、与えられるパラメータに柔軟性を与えるものです。ドキュメントだけでは正体が定かにはならないですが、 gcr.io/dataflow-templates-base/java11-template-launcher-base をベースとした Docker image を用いて Dataflow ジョブを実行するようになります。テンプレートジョブの構築手順は「従来のテンプレート」に似ており FlexTemplate によるテンプレートの作成と、実行の 2 つのフェーズがあります。前者では前述の Docker image を GCR に push して、かつその Docker image で解釈可能なパラメータを示した metadata.json を GCS に upload します。 Docker image の build & push は従来のテンプレートでは存在しなかった手順ですね。https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates#creating_a_flex_template後者では実際にパラメータを与えてジョブを実行します。cloud.google.comこのジョブの実行時に興味深いことに、ジョブの launch 用に GCE インスタンスが開始され先程作成した Docker image が実行されることが見て取れます。この Docker コンテナの動作がよくわからないところですが、内部的にはこのタイミングから Dataflow ジョブのステージングと実行を開始してくれるものと考えられます。#cloud-configs...    ExecStart=/usr/bin/docker run -v /var/log/dataflow/template_launcher:/var/log/dataflow/template_launcher gcr.io/xxx/samples/dataflow/streaming-beam-sql --template-container-args='{\"consoleLogsLocation\":\"gs://xxx/staging/template_launches/xxx/console_logs\",\"environment\":{\"region\":\"us-central1\",\"serviceAccountEmail\":\"xxx\",\"stagingLocation\":\"gs://xxx/staging\",\"tempLocation\":\"gs://xxx/tmp\"},\"jobId\":\"xxx\",\"jobName\":\"streaming-beam-sql-20200512-004931\",\"jobObjectLocation\":\"gs://xxx/staging/template_launches/xxx/job_object\",\"operationResultLocation\":\"gs://xxx/staging/template_launches/xxx/operaton_result\",\"parameters\":{\"inputSubscription\":\"test\",\"outputTable\":\"foo:bar.baz\",\"stagingLocation\":\"gs://xxx/staging\",\"tempLocation\":\"gs://xxx/tmp\"},\"projectId\":\"xxx\"}'...さてこの FlexTemplate で何が嬉しいのかというと、大きな差異として実行時パラメータを ValueProvider で包む必要が無くなった点があります。FlexTemplate を利用したジョブのサンプルがいくつか公開されているのですが、この内の PipelineOptions の getInputSubscription() など、実行時に指定可能なパラメータでありつつも ValueProvider 型で扱われていないことが見てとれます。github.comこれにより、従来のテンプレートでは課題になっていた ValueProvider を受け取ってくれないような値を実行時に変更したいだとか、パイプラインの構造が変わるようなパラメータを渡したいような部分に効果を発揮するものと思われます。とは言ってもデメリットもあり、ジョブが Queue に入ってから Running の状態になるまで数分待たされる（ステージングのタイミングが遅延したのなら妥当に思えるが）だとか、今のところ Streaming Engine や FlexRS のサポートがないとか課題が存在します。終わりに軽く触れてみた限り FlexTemplate はシンプルな仕組みながら Dataflow のテンプレートに相当な柔軟性を持たせることができそうです。FlexTemplate にどこまでを期待して、どのように従来のテンプレートと棲み分けすべきかのか（上位互換の位置付けなのか？）、今後のサポート具合など気になるところですね。","link":"https://syucream.hatenablog.jp/entry/2020/05/12/010008","isoDate":"2020-05-11T16:00:08.000Z","dateMiliSeconds":1589212808000,"authorName":"syucream","authorId":"syucream"},{"title":"Schema Registry について書いていく: Confluent Schema Registry の Protocol Buffers & JSON Schema サポート","contentSnippet":"先日リリースされた Confluent Platform 5.5 より Protocol Buffers と JSON Schema のサポートが入ったようです。www.confluent.io以前 5.4 を対象に、 Schema Registry を中心に色々記載してみましたが、今回は 5.5 で入ったこの差分を追跡してみます。syucream.hatenablog.jpConfluent Control Center 上でスキーマを管理する定番の Confluent Control Center を使って、ブラウザ上で Schema Registry の設定周りを試してみましょう。試しに protobuf でスキーマを登録してみた状態が以下のとおりです。protobuf において .proto ファイルに記述していくスキーマを登録できるようになっています。JSON Schema の場合は例えば以下のようになります。互換性チェックの挙動を試してみるこれだけでは面白くないので protobuf のスキーマを更新していってみます。まず uint32 age = 3; フィールド追加を行ってみます。フィールド追加は特に互換性を壊しません。というわけでスキーマの更新も無事に行なえます。syntax = \"proto3\";message value_protobuf {  uint64 id = 1;  string name = 2;  uint32 age = 3;}今度は field number を変更してみます。早速さきほど追加したフィールドを  uint32 age = 42; に書き換えてみました。この変更もリスクはありそうですが、スキーマ更新は受け入れられます。syntax = \"proto3\";message value_protobuf {  uint64 id = 1;  string name = 2;  uint32 age = 42;}さらにフィールドの削除やフィールド名の変更も受け入れられます。syntax = \"proto3\";message value_protobuf {  uint64 id = 1;  string fullname = 2;}これらの変更は互換性を壊すものではないのでしょうか？ Confluent Schema Registry での protobuf の互換性チェックの仕様は以下の通りになっています。docs.confluent.ioおおむね protobuf の wire format の解釈に影響を及ぼさず、デシリアライズは無事行えるような内容であれば互換性が維持されると見るようです。フィールド名の変更に対する言及は明記されていませんが、これも wire format の解釈に影響しないからだと思われます。・・・ただ実際にはデシリアライズには成功するとしても後続のデータ処理が影響を受けないかどうかは保証されないでしょうし注意が必要です。フィールドの追加や削除は予期せぬ値のデフォルト値化、フィールド名のリネームはフィールド名を意識した後続処理で問題が発生するかも知れません。ここから明確に互換性が無い変更も行ってみます。このフィールドの型 uint32 を string にしてみます。この場合は予想通り互換性が無いとされてスキーマ更新が拒否されます。// 筆者はこの他にも JSON Schema のスキーマ変更も試してみたのですが、互換性チェックの挙動が読めず諦めてしまいました。protobuf の扱い周りの実装を読んで見る5.5 でサポートされたデータフォーマットのバリエーションと各フォーマットの扱いはどうなっているのでしょう。せっかくなので前節で触れた protobuf 周りの実装を追ってみます。protobuf 対応した Serializer/Deserializerドキュメントから、データフォーマット毎に Serializer, Deserializer の実装が異なるようです。まずはここをエントリポイントとしてみます。docs.confluent.ioAvro と同様 AbstractKafkaProtobufSerializer, AbstractKafkaProtobufDeserializer に、 protobuf のデータである Message のサブクラスのオブジェクトのシリアライズ・デシリアライズ処理と schema registry とのやり取りの処理が実装されています。ここで protobuf のスキーマ情報は公式の Descriptor としてではなくラップされた型として扱われます。また protobuf は import 文で他の .proto ファイルを読み込むことができるのですが、この依存関係を解決するロジックも持ち合わせています。面白い点として Serializer は protobuf の、 protoc で生成された具体的なメッセージクラス (Avro でいう SpecificRecord みたいなもの)を扱い、 Deserializer では DynamicMessage (Avro でいう GenericRecord みたいなもの) を扱うことです。producer は .proto の記述に従ったメッセージクラスが使えますが、それを consumer が持っている保証はないため妥当な扱いだと思われます。複数データフォーマットをサポートしたスキーマの表現5.4 では Schema Registry として考慮していたスキーマが Avro しかありえなかったので、 Schema Registry としては Avro の Schema クラスを扱うような実装になっています。これが 5.5 では ParsedSchema インタフェースという一段抽象化された構造で扱われます。protobuf のスキーマを表現する ProtobufSchema、 Avro の AvroSchema, JSON Schema の JsonSchema はそれぞれ ParsedSchema を実装しています。個々の ParsedSchema 実装の中でスキーマ固有の処理、互換性チェックや protobuf で言う descriptor の扱いなどを担います。おわりに簡潔に追っただけですが、妥当な進化を遂げたような Confluent Platform 5.5 でした。protobuf はコード事前生成を想定した利用シーンが多いですが、 Confluent Platform でのサポートなど利用シーンが増えてくると活用できる機会がさらに増えるかも知れません。また今回複数データフォーマットのサポートが入った以外にも ksqldb という KSQL の進化系？のような機構が増えて Confluent Platform も進化を続けている印象を受けますね。","link":"https://syucream.hatenablog.jp/entry/2020/05/03/013316","isoDate":"2020-05-02T16:33:16.000Z","dateMiliSeconds":1588437196000,"authorName":"syucream","authorId":"syucream"},{"title":"protobuf のシリアライズ済みバイナリを無理やり読む","contentSnippet":"Protocol Buffer wire format についてProtocol Buffer でシリアライズされた後のバイナリのレイアウトの仕様は wire format の仕様という形で独立してドキュメントが用意されています。この wire format の仕様は見ればわかる通りそれほど記述量が多くなく、それでいて互換性を気にしつつ、かつ拡張の邪魔にならないような配慮がされています。developers.google.comwire format として特に重要な点は以下の通りだと個人的には考えます。整数型に対するデータサイズがなるべく小さくなるような工夫がされている複雑な型はバイト配列に押し込める。（多くのシリアライゼーションフォーマットと同じように）長さが指定されたバイト配列として表現フィールドの順序が任意にできる。のでスキーマ更新において順序を意識しなくてよくなる。wire format では各フィールドはフィールド番号と wire format 上での型で識別されます。これらを元に、 .proto ファイルでどう記述されていたのかに対応してより具体的な型として解釈したりフィールドの名前を取得したりできるわけです。例えば field number が 10 、 wire type が 2(バイト配列) であった場合に、 .proto では string name = 10; という対応する field number があるならフィールド名は name でありバイト配列は文字列として解釈できそうだと判断できます。とはいえシリアライズ済みバイナリを頻繁に扱ってくると、ましてや複数のシステムでデータ交換をし出すと時には .proto や descriptor 、ライブラリなしに内容を確認したい時があるかも知れません。特にデータ交換を行う場合システム間で持つスキーマが異なる場合にはこの希望は大きくなるかもです。またパースエラーが起こる箇所を絞り込みたいなどの要望から、スキーマを用いた具体的なデータの解釈をなるべく遅らせたい場合にこの戦略は有効かも知れません。そんなことを考え、 Go で無理やりシリアライズ済みバイナリを、それ単体の持つ情報だけで解釈してみる試みをしました。Protocol Buffer without schemaというわけで Go で schema less で protobuf のデータを無理やり解釈するコードを書いてみました。github.comwire format のフィールドの解釈は protowire という既存のモジュールがあり、詳細な部分は概ねこれで行えます。wire format の解釈の上で面倒くさい点は可変長になりうる varint の値や ZigZag encoding されうる signed integer あたりですがこのあたりは実装されてくれています。godoc.orgバイト列の詳細な解釈は descriptor の情報が無いと正確な値は取れません。ここでは雑に、思いつく範囲の解釈を全部試して wire format としては valid な値をすべて返すようにしています。具体的には文字列とバイト列、ネストされたメッセージ、整数型の repeated な値などです。そのほかにも Protocol Buffer としてはバイト列で map 型が表現できるのですが、これについては wire format やその他のドキュメントに詳細な仕様がなく（うまく見つけてなくてどこかに存在するかも？）一旦諦めています。Go の実装的にはバイト列の中に更にネストされたメッセージに似てフィールド番号や wire type が指定されたフィールド列が存在して、フィールド番号が 1 であれば map の key 、 2 であれば value となるようです。github.comこれを用いて以下のような .proto のメッセージをこんな様な値を設定してシリアライズした時の               msg := &protosl.Example{                    Uint64Val:  1,                    StringVal:  \"testing\",                    Fixed64Val: 11,                    Fixed32Val: 111,                    EnumVal:    protosl.Example_ONE,                    ChildVal: &protosl.Child{                        V: 1,                    },                    RUint64Val: []uint64{2, 3},                    RStringVal: []string{\"aaa\", \"bbb\"},                    // RFixed64Val: []uint64{22, 33}, TODO repeated fixed isn't supported                    // RFixed32Val: []uint32{222, 333}, TODO repeated fixed isn't supported                    REnumVal: []protosl.Example_Num{protosl.Example_ZERO, protosl.Example_ONE},                    RChildVal: []*protosl.Child{                        {                            V: 2,                        },                        {                            V: 3,                        },                    },                }シリアライズ後のバイナリを食わせると、かなり冗長にはなりますが以下のように解釈できます。$ echo -n \"\\x08\\x01\\x12\\x07\\x74\\x65\\x73\\x74\\x69\\x6e\\x67\\x19\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x25\\x6f\\x00\\x00\\x00\\x28\\x01\\x32\\x02\\x08\\x01\\xaa\\x06\\x02\\x02\\x03\\xb2\\x06\\x03\\x61\\x61\\x61\\xb2\\x06\\x03\\x62\\x62\\x62\\xca\\x06\\x02\\x00\\x01\\xd2\\x06\\x02\\x08\\x02\\xd2\\x06\\x02\\x08\\x03\" | protosl{\"1\":1,\"101\":{\"__bytes\":\"AgM=\",\"__packed\":[2,3],\"__string\":\"\\u0002\\u0003\"},\"102\":[{\"__bytes\":\"YWFh\",\"__packed\":[97,97,97],\"__string\":\"aaa\"},{\"__bytes\":\"YmJi\",\"__packed\":[98,98,98],\"__string\":\"bbb\"}],\"105\":{\"__bytes\":\"AAE=\",\"__packed\":[0,1],\"__string\":\"\\u0000\\u0001\"},\"106\":[{\"__bytes\":\"CAI=\",\"__message\":{\"1\":2},\"__packed\":[8,2],\"__string\":\"\\u0008\\u0002\"},{\"__bytes\":\"CAM=\",\"__message\":{\"1\":3},\"__packed\":[8,3],\"__string\":\"\\u0008\\u0003\"}],\"2\":{\"__bytes\":\"dGVzdGluZw==\",\"__packed\":[116,101,115,116,105,110,103],\"__string\":\"testing\"},\"3\":11,\"4\":111,\"5\":1,\"6\":{\"__bytes\":\"CAE=\",\"__message\":{\"1\":1},\"__packed\":[8,1],\"__string\":\"\\u0008\\u0001\"}}バイナリを直接確認できると何かしら便利、フィールドとの対応付けは後で行うので・・・という時に役のに立つかも知れません。","link":"https://syucream.hatenablog.jp/entry/2020/04/26/010710","isoDate":"2020-04-25T16:07:10.000Z","dateMiliSeconds":1587830830000,"authorName":"syucream","authorId":"syucream"},{"title":"原則 WFH 勤務が開始して二ヶ月が経過した","contentSnippet":"タイトルの通りで単なる日記なのですが、今の心境を赤裸々に綴っておくと後で振り返れると思いつらつら書きます。なおこのエントリは個人の意見ですし、同じ会社同じチームでも受け取り方や課題感はだいぶ差異が出るんじゃないかなーと思っています。二ヶ月前からこれまで今年 2 月 19 日に新型コロナウイルス蔓延を受けて、勤め先が以下のようなプレスを出しました。about.mercari.comこの段階で大規模にリモートワークに転換する企業はそう多くなく、このときは代表例としては GMO さん、ついでドワンゴさんあたりが原則リモートワークで業務をすることを宣言していた記憶があります。東京都内のこの時の状況としては、少ないながらも感染事例が挙げられつつあり、会社として特に方針が無くともチームとして満員電車の通勤を避けたいななどと話していたりしました。そういう情勢だったので会社として早めに方針が打ち出されたのはいち従業員としては行動方針を決めやすくもありありがたいものでした。チーム的には Slack ベースのコミュニケーションを盛んにやっており、リモート勤務に必要な設備もチームメンバーがそれほど障壁なく行えたのは幸いでした。WIAS という勤怠管理システムのお世話にもこの頃からなり始めました。tech.mercari.comさて原則 WFH はそれほど課題なく継続されて、一ヶ月が経過してお花見シーズンになる頃には世間的にも緩みが出てきて、チーム的にも週一くらいで出社して社会復帰してみよう（？）という雰囲気が出てきました。その矢先に永寿総合病院における院内感染が発生して（通勤経路や生活圏的にこのあたりで僕は余計に危機感を覚えたのですが）、緩み始めたムードは一点します。より強く WFH を求められるようになり、世間のニュースも仄暗いものばかりになってきます。その傾向は緩むどころか悪化するばかりで、やがて緊急事態宣言に至り、本稿執筆時は多くの飲食店等が活動を休止して街はすっかり息をひそめています。WFH を続けてみた所感そんな具合で世間的には比較的早く原則 WFH の働き方に切り替えて二ヶ月経過した訳ですが。WFH だから被った被害ややりにくさというのはそれほど無いと考えています。ただしホワイトボードなどを使って集団でブレストしたり、ふらっと立ち話風に同僚に声かけて相談するという行動がしにくくなったのは損失ではあります。ただ致命的ではないとも。また僕の場合は部屋が狭く（自宅の快適さより通勤の快適さに重きを置いていた）、働くのにあまり適さない自宅環境だったのも大きな課題でした。今回の件で一番の問題は WFH で勤務することそれ自体でなく新型コロナウイルス蔓延の先行きが見えず、時には大きく状況が悪化することがあった部分だと思います。働き方というか働くことというか、それ以前に世間を取り巻く状況が悪く、精神衛生に良くないのがもっとも辛い。とりあえずこの先生きのこるには直近試しているのはとにかく作業環境の改善です。1K 限界一人暮らし部屋はそのままでいると漏れなく仕事場所がベッドルームと同一存在になります。これに耐えられる人は良いのでしょうが、僕は耐えられませんでした。この課題に対して理想的にはさっさと引っ越し！したいところですが、不要不急の外出を自粛するよう求められた我々にできることは限界があります。というわけで妥協案としてクローゼットの中のものを外に追いやるか思い切って処分してスペースを構築して、クローゼット内部を作業場所にするようにしました。部屋が狭く作業スペースを確保しにくいことについカッとなってクローゼットを簡易作業スペースにしてしまった pic.twitter.com/H5neupDO7t— しゅー　くりーむ (@syu_cream) 2020年4月8日  このようなクローゼットの活用の仕方には「クロッフィス」と呼ばれることがあるようです（どれくらい知名度があるかは知らん）weboo.link今ではカーテンで簡易仕切りを設けたり造花で草を生やしたり最適化を進めています。クローゼットを潰して作った作業スペース、謎の進化をし始めている pic.twitter.com/ZPtbcKbfyM— しゅー　くりーむ (@syu_cream) 2020年4月20日  そして精神衛生に悪い状況であることは、もうしょうがない。しばらくは今回の騒動が起こる前と同じようには働けないと、諦めることにしています。逆にこの状況の変化で生産性が向上するのならそれはとてもすごいこと。だけどそうそう起こらないこと。つけ加えるなら、メンタルが弱った自覚が出てるうちにさっさと有給を取るなりして休んでしまうのが良いとも思っています。ねてればなおる、しらんけど。","link":"https://syucream.hatenablog.jp/entry/2020/04/20/232546","isoDate":"2020-04-20T14:25:46.000Z","dateMiliSeconds":1587392746000,"authorName":"syucream","authorId":"syucream"},{"title":"BigQuery SQL UDF の挙動を色々確認する","contentSnippet":"BigQuery ではユーザ定義関数(UDF) を作ることができる。これを使って、よく使われる式や関数呼び出しの組み合わせを名前付けして再利用できる。cloud.google.comUDF は SQL と JavaScript の二種類の言語による記述が可能で、後者は色々なトリッキーな利用例も世に出ているので知っている人も多いかも知れない。例えば WebAssembly のコードを実行するとかである。medium.com本記事では 地味な方 素直な SQL での記述による UDF について色々挙動を確認してみる。戻り値の型を推論してくれるUDF は例えばシンプルなものだと以下のように記述できる。CREATE TEMP FUNCTION  zero()  RETURNS INT64 AS (0);SELECT  zero()ここで戻り値の型は省略することができる（勝手に解決してくれる）CREATE TEMP FUNCTION  zero()  AS (0);SELECT  zero()STRUCT 型の戻り値も推論してくれる。が、もちろん各フィールド名の情報はないのでダミーの値が補完されてしまう。CREATE TEMP FUNCTION  user()  AS ((1, \"taro\"));SELECT  user()[  {    \"f0_\": {      \"_field_1\": \"1\",      \"_field_2\": \"taro\"    }  }]RETURNS で型を明示するとそこに記述されたフィールド名を拾ってくれる。CREATE TEMP FUNCTION  user()  RETURNS STRUCT<id INT64, name STRING>  AS ((1, \"taro\"));SELECT  user()[  {    \"f0_\": {      \"id\": \"1\",      \"name\": \"taro\"    }  }]UDF から UDF を呼ぶ際はすでに定義済みである必要があるUDF から別の UDF を呼び出すこともできる。CREATE TEMP FUNCTION  callee() AS (42);CREATE TEMP FUNCTION  caller() AS (callee());SELECT  caller()この定義順序を逆にするとクエリの実行が通らなくなる。 Function not found: callee; failed to parse CREATE [TEMP] FUNCTION statement at [5:16] というエラーが出てしまう。CREATE TEMP FUNCTION  caller() AS (callee());  -- <- Function not found :(CREATE TEMP FUNCTION  callee() AS (42);SELECT  caller()UDF の再帰呼び出しをすることもできない。この例では Function not found: fib; failed to parse CREATE [TEMP] FUNCTION statement at [10:7] と怒られてしまう。CREATE TEMP FUNCTION  fib(n INT64) AS (  IF    ( n > 2,      fib(n - 1) + fib(n - 2),      1 ) );SELECT  fib(2)だったら一度ダミーの UDF を定義してみよう！ということで一時 UDF じゃなく永続 UDF を、一度ダミーのものを定義したあと REPLACE してみる。このクエリはそれ自体は valid と言われ実行可能である。-- dummyCREATE OR REPLACE FUNCTION  udfs.fib(n INT64) AS (1); CREATE OR REPLACE FUNCTION  udfs.fib(n INT64) AS (  IF    ( n > 2,      udfs.fib(n - 1) + udfs.fib(n - 2),      1 ) );SELECT  udfs.fib(2)が、これは実行してみると Query error: Too many nested views/persistent user-defined functions or possible circular reference of views/persistent user-defined functions referenced in query. Only 16 levels of nested views/persistent user-defined functions are allowed. at [11:1] と怒られてしまう。 UDF では 16 チェーンまでしか UDF 呼び出しをできない制限がありそれに引っかかっているように見えるエラーメッセージである。この例では udfs.fib(2) を呼び出してもそもそも再帰呼び出しされない気がするが・・・。cloud.google.com頑張ればループ処理を書けるUDF では前述の通り再帰呼び出しできず、ループするような制御構文もサポートされていない。が、 ARRAY 型の値と ARRAY 型関連関数を使うとループ、リストの各要素に対する繰り返し処理的なことができる。ミソとなるのがみんな大好き UNNEST() でテーブルを手にいれることができることと、 ARRAY() の引数はサブクエリを取れる辺りにある！CREATE TEMP FUNCTION  array_twice(n INT64) AS (ARRAY(    SELECT      AS STRUCT (      SELECT        ARRAY_AGG(vv)      FROM (        SELECT          v * 2  -- <- like a map function        FROM          UNNEST(GENERATE_ARRAY(1, n)) AS v) AS vv)));SELECT  array_twice(100)テーブルを参照することもできる上記より SELECT 文が打てることを確認できたので FROM 句に実テーブルを渡してみる。このクエリも合法で実行することができる！CREATE TEMP FUNCTION  do() AS (ARRAY(    SELECT      AS STRUCT (      SELECT        ARRAY_AGG(vv)      FROM (        SELECT          LENGTH(repo_name)  -- <- like a map function        FROM          `bigquery-public-data.github_repos.licenses` -- <- refer to a table        LIMIT          10) AS vv)));SELECT  do()ドキュメントにテーブル参照に関する記述があるので、この挙動は一応想定されたもののはず。（もしかしたら他のまっとうな参照方法があるのかも）一意の UDF とテーブル参照を合わせたクエリあたりの最大数 - 1,000。完全な展開後に、UDF ごとに一意のテーブルと UDF を合わせて 1,000 個まで参照できます。cloud.google.com","link":"https://syucream.hatenablog.jp/entry/2020/03/08/024100","isoDate":"2020-03-07T17:41:00.000Z","dateMiliSeconds":1583602860000,"authorName":"syucream","authorId":"syucream"},{"title":"Apache Parquet の Logical Types に関するメモ","contentSnippet":"去年末になりますが、 embulk-output-s3_parquet という Embulk Plugin にて Logical Type をサポートするためのパッチを書き、マージしていただきました。github.com個人的な主な目的は timestamp 型をサポートすることでした。 Athena や BigQuery から参照するにあたり timestamp 型として扱えるか否かで使い勝手は大きく変わります。Logical Type としての扱いを期待するかを設定に書く必要はあるものの、割と簡素な設定で timestamp 型対応ができるようになったのではないかと思います。さてこの Logical Type ですが、 2020 年 2 月現在 2 種類の仕様が混在してかつ古い仕様が事実上スタンダードになっている状態です。自分は最初混乱したのですが、この不幸の連鎖を止めたいので、本記事に自分が把握した状態を記録しておきます。古い仕様 \"Converted Types\"ここまで一口に \"Logical Types\" といってしまったのですが、ドキュメント上は古い仕様は Converted Types, 新しい仕様が Logical Types と区別されます。そして現在使われる Logical Types としてはこの Converted Types が一般的でしょう。github.com前提として、 Parquet がサポートする型 は以下のものになります。BOOLEANINT32INT64INT96FLOATDOUBLEBYTE_ARRAYConverted Types ではこれらの型に別解釈を与えることができます。これは OriginalType に定義されています。（このクラス名からか、 JIRA や GitHub 上のやり取りを見ると Converted Types のことを Original Types と呼ぶこともあるっぽい？ようです。 Java 実装に限った話かも）MAP,LIST,UTF8,MAP_KEY_VALUE,ENUM,DECIMAL,DATE,TIME_MILLIS,TIME_MICROS,TIMESTAMP_MILLIS,TIMESTAMP_MICROS,UINT_8,UINT_16,UINT_32,UINT_64,INT_8,INT_16,INT_32,INT_64,JSON,BSON,INTERVALembulk-output-s3_parquet でサポートする timestamp 型はここに現れる TIMESTAMP_MILLIS, TIMESTAMP_MICROS になります！また Parquet では文字列型がこの Converted Types の UTF8 OriginalType を使って表現されます。そのため、Parquet の Converted Types は例えば Avro における Logical Types よりは多くのユーザが触れる機会がある概念かも知れません。新しい仕様 \"Logical Types\"新しい Logical Types は UUID やナノ秒精度 timestamp などさらにリッチな型をサポートしていたり、 Builder パターンでのスキーマ構築をサポートしていたりと様々な更新を含みます。Converted Types との互換性も考慮されており、互換がある表現の場合は変換メソッドも提供されます。これは Parquet 1.11.0 からサポートされているのですが、 maven central repository に上がったのが 2019 年 11 月でありまだ歴史が浅い機能といえます。ミドルウェアの対応としても、 Hive でサポート開始がされ始めた ようですが Spark はまだ だったり普及するのにはまだ時間を要しそうです。BigQuery などクラウドサービスにおいても、スケジュールは定かではないですが、サポートするにしてももうしばらく時間が必要かも知れません。そんな訳で 2020 年 2 月現在、まだ多くの場合は Converted Types を利用することになると思うのですが、将来的に Logical Types に置き換えることや変換メソッドがあることを知っておくと良いでしょう。またドキュメントは新しい Logical Types に関する記述の方が増えていくと思われ、現行２つの仕様がありドキュメントの記述と自分が必要としてい仕様がどちらのものか意識することがしばらく必要かも知れません。","link":"https://syucream.hatenablog.jp/entry/2020/02/24/030836","isoDate":"2020-02-23T18:08:36.000Z","dateMiliSeconds":1582481316000,"authorName":"syucream","authorId":"syucream"},{"title":"Schema Registry について書いていく その1: Confluent Schema Registry","contentSnippet":"分散アプリケーション間のメッセージングやログ収集基盤において、しばしばスキーマの扱いは便利である反面頭を悩ませる種になります。スキーマを厳密に定義して、 Protocol Buffers や Avro などのシリアライゼーションフォーマットを用いることで、メッセージのサイズの減少やメッセージの扱いの安全性を実行時より早く検出できる機会が増えます。しかしスキーマの互換性に気をつけたり、メッセージをシリアライズ・デシリアライズするアプリケーション間でスキーマ情報をどう共有するかの課題も発生します。この課題は、昨年発売された \"データ指向アプリケーションデザイン\" でも \"4.1.4.3 そもそもライターのスキーマとは何なのか？\" などで触れられています。世の中にはこの課題を解決するための \"スキーマレジストリ\" と呼ばれるソフトウェア、サービスがいくつか存在します。残念ながら日本語の資料があまり無いようにも感じますし、筆者の気が済むか飽きるまで様々なスキーマレジストリを探索してみようかと思います。第一回は Confluent Schema Registry について調べてみます。（残念ながら筆者は Confluent Schema Registry を実務で触れた経験がなく、なにか内容に誤りがある可能性があります）Confluent Schema Registry とはConfluent Schema Registry は、マネージドな Apache Kafka を提供する Confluent 社 が開発したスキーマレジストリです。docs.confluent.io以下のような機能を持っています。Apache Kafka に組み込む想定で、 Apache Avro のスキーマを管理Subject 名というスキーマレジストリ用のスキーマ解決の為のキー情報を持つKafka の専用 topic にスキーマ情報を格納する互換性チェックの機構を持つREST API を提供する管理用の WebUI でスキーマ管理を行えるApache Avro については以前本ブログで触れましたし世の中にドキュメントが多数存在するのでここでは別段触れません。スキーマは Subject という識別に用いる名前とバージョン番号で管理されます。Subject をどう決めるかはいくつかのオプションがあり、デフォルトでは topic 名になっています。topic 名を用いる場合はある topic に流れるレコードのスキーマは一つに限定され、複数の異なるスキーマのレコードを流すことは出来ません。これを行いたい場合は他にレコード名もしくは topic 名とレコード名の組み合わせを Subject として用いることになります。スキーマ情報の格納先がそれもまた Kafka であるのはちょっとユニークな点と言えそうです。デフォルトではシングルパーティションの _schemas topic にスキーマ情報を格納するようです。docs.confluent.ioスキーマレジストリという機構の悩ましい点として、それ自体がメッセージバスと同等くらいの可用性を求められることが挙げられます。メッセージバスが生きていてもスキーマレジストリが死んでいるなら、シリアライズする時に用いたスキーマを何処にも格納できず、デシリアライズする側のシステムに伝える術も失うためです。Confluent Schema Registry ではスキーマレジストリの REST API を提供する HTTP サーバの可用性の懸念は残りますが、ストレージは Kafka それ自体と運命共同体になるため考える課題が一つ減りそうです。Confluent Schema Registry は更にスキーマの更新問題にも解決策をもらたします。アプリケーションを稼働させていくうちにスキーマを更新したくなるシーンが登場するのは自然なことでしょう。この際に Confluent Schema Registry は Kafka の Producer が作成した Avro のスキーマは解決してくれますが、 Consumer 側にも配慮する必要があります。例えば Consumer が必要としているフィールドがある日 Producer が送信するレコードから無断で削除されたなら、 Consumer 側で何らかの問題が出るかもしれません。この問題に配慮するため、 Confluent Schema Registry ではスキーマの互換性チェックの機構を設けています。互換性には前方互換性、後方互換性、完全互換性の 3 タイプが設けられています。docs.confluent.io前方互換性は Producer が Consumer より先にスキーマを更新するパターンで求められる互換性です。この場合フィールドの追加もしくは optional 、すなわちデフォルト値を持つフィールドの削除が許されています。これらの変更が行われても Consumer は、自分が知らない追加フィールドは無視する、削除されたフィールドはデフォルト値で埋めることができます。後方互換性は Consumer が Producer より先にスキーマを更新するパターンで求められる互換性です。この場合は前述の逆で、 optional なフィールドの追加とフィールドの削除が許されています。この互換性タイプが Confluent Schema Registry のデフォルトの互換性モードであり、現実世界で気を使われることが多いものであると思います。完全互換性は Producer と Consumer どちらが先にスキーマを更新しても良いパターンで求められる互換性です。この場合は制約が最も弱いようなスキーマ更新、つまり optional なフィールドの追加・削除が許されるようになります。この互換性タイプは制約が弱いため扱いにも困るケースが出るかも知れませんが、 Confluent のプラットフォーム以外のミドルウェアやキャッシュ機構が存在したり処理するレコードの順序が保証されなかったりするケースに、互換性の問題で Consumer 側でエラーが発生するケースを低減できそうです。Confluent Schema Registry はスキーマを更新しようとする際に、求められるタイプに従って互換性をチェックする機能を提供します。REST API は実際に Confluent Schema Registry に対応した SerDe でスキーマ操作を行う際に使われる API にもなります。手動で REST API を叩いてスキーマ登録を行うなどをしても良いでしょう。この API の仕様は以下にまとまっています。docs.confluent.io管理用の WebUI については実際に触れてみた方が話が早そうです。次に触れてみましょう。Confluent Schema Registry に実際に触れてみるクイックスタート公式で Docker イメージと docker-compose yml ファイルが提供されており、とりあえず動かす敷居が低めになっています。docs.confluent.ioこのドキュメントに沿って Docker コンテナを動かした後にブラウザ上で http://localhost:9021/ にアクセスしてみます。上手く動いていれば以下のような管理画面を閲覧できるはずです。Avro のスキーマを登録してみるここでは実験用に hello topic を作成して、この topic に流すレコードの value のスキーマを設定してみようと思います。ここではバージョン 1 のスキーマとして以下を与えてみます。{  \"name\": \"Hello\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    },    {      \"name\": \"name\",      \"type\": \"string\"    }  ]}登録したスキーマを WebUI 上で確認できます。一度スキーマを登録すると互換性モードを選べるようになります。ここでは一つ前のバージョンのスキーマと後方互換性を保つモードである Backward に設定しておきます。Avro のスキーマを更新してみる; 互換性がある場合先程登録したスキーマを更新してみようと思います。ここでは年齢を指定する想定で {\"name\": \"age\", \"type\": [\"null\", \"long\"], \"default\": null} という nullable なフィールドを追加してみます。{  \"name\": \"Hello\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    },    {      \"name\": \"name\",      \"type\": \"string\"    },    {      \"name\": \"age\",      \"type\": [\"null\", \"long\"],      \"default\": null    }  ]}このスキーマ変更は optional なフィールド追加に相当して後方互換性チェックをパスすることができます。無事 WebUI からも新しいバージョンのスキーマを閲覧することができています。diff 表示も出来ていい感じです。Avro のスキーマを更新してみる; 互換性がない場合ここから後方互換性が無い変更をしてみます。よく考えたらこのレコードをアプリケーション上で作成した時刻の timestamp 値を保持したくなってきました。ということで optional でないフィールド {\"name\": \"created_at\", \"type\": \"long\", \"logicalType\": \"timestamp-millis\"} を追加してみます。{  \"name\": \"Hello\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    },    {      \"name\": \"name\",      \"type\": \"string\"    },    {      \"name\": \"age\",      \"type\": [\"null\", \"long\"],      \"default\": null    },    {      \"name\": \"created_at\",      \"type\": \"long\",      \"logicalType\": \"timestamp-millis\"    }  ]}しかしながら先述の通り後方互換性を保つには optional なフィールド追加は許されません。この場合 Confluent Schema Registry が互換性が保てない旨のメッセージを出してスキーマの更新が拒否されます。このように Confluent Schema Registry では、スキーマの管理と共に、求められる互換性のタイプに従ってそれを維持するようチェックをかけてくれます。本当はスキーマ管理を .avsc ファイルなどで保存して CI で互換性チェックし、パスすればスキーマ更新するなど人間による直接の更新を避けるのが望ましいかもですが、この WebUI と互換性チェック機構で人間による直接管理もある程度運用に耐えるかもしれません。余談; _schemas topicWebUI 上では _schemas topic の様子も確認することができます。log.cleanup.policy が delete ではなく compact が設定されており、これにより時間経過等によるレコードの削除を抑制しています。Confluent Schema Registry の実装を覗いてみるConfluent Schema Registry のコードはGitHub で公開されています。折角なのでどんな実装になっているか少し覗いてみます。ちなみにここでは網羅性などは全く考慮していません。筆者が気になるポイントを勝手にかいつまんでいます。github.comここで紹介する内容は v5.4.0 時点のものとなります。avro-serializer: Schema Registry と通信した結果でシリアライズ・デシリアライズを行うこの module に存在するのが、実際に producer, consumer を実装する際に properties に指定する serializer, deserializer になります。シリアライズのコアの実装が AbstractKafkaAvroSerializer の serializeImpl() に存在します。自動登録する設定になっているなら Avro スキーマをスキーマレジストリに登録し、その後 Kafka の topic に送信するメッセージをシリアライズします。この時のバイナリレイアウトなのですが、 この Serializer 実装特有のフォーマットになっており Avro のレコードの手前に 8 バイトのマジックナンバーとスキーマの ID が埋め込まれています。Avro のレコードのバイナリはおなじみ SpecificDatumWriter, ReflectDatumWriter, あるいは GenericDatumWriter で書き出します。docs.confluent.ioこれに対応してデシリアライズ側の実装は AbstractKafkaAvroDeserializer クラスに多くのロジックが持たれています。まずは deserialize() ですがリーダ側のスキーマが渡せるようになっており、デシリアライズの結果をライター側とは異なる（しかし互換性はある）型に展開できるようになっています。ライター側のスキーマは勿論スキーマレジストリ経由で解決できます。この辺りの実装は  AbstractKafkaAvroDeserializer の内部クラスの DeserializationContext クラスで行われます。このクラスに渡された、デシリアライズ対象のバイナリは、先頭 8 バイトにマジックナンバーとスキーマ ID が存在するはすです。まずマジックナンバーの存在チェックを行った後にスキーマ ID を取り出し、この ID をスキーマレジストリに問い合わせて Avro のスキーマ情報を取得します。その後はやはりおなじみ SpecificDatumReader, ReflectDatumReader, GenericDatumReader で Java のオブジェクトとして読み出します。これで概ね Avro のレコードのシリアライズとデシリアライズの流れが掴めますが、スキーマの登録と取得がどのように成されているのでしょうか。先述の Serializer, Deserializer の共通の親クラスとして AbstractKafkaAvroSerDe があり、このクラスがデフォルトでは CachedSchemaRegistryClient というスキーマレジストリに対するクライアントを保持しています。このクライアントの実装がどうなっているのか、次はスキーマレジストリとの通信を行っている client module も覗いてみます。client: Schema Registry のクライアントSchemaRegistryClient インタフェースを実装したクラスが実際に Schema Registry と通信しています。先述の CachedSchemaRegistryClient はその 1 実装であり、スキーマや ID, バージョン情報のキャッシュと RestService というクラスのメンバを持ちます。そして RestService が実際に HTTP リクエストをスキーマレジストリの REST API に送信する、例えばスキーマの登録時に /subjects/:subject/versions に POST リクエストを送る、というような格好になります。core: Schema Registry のコア実装もうひとつ、スキーマレジストリの REST API サーバを提供する core モジュールも気になる存在です。REST API の実装やスキーマ情報などの永続化をどのように行っているのでしょうか。スキーマレジストリの REST API サーバは Jetty + Jersey で実装されているようです。API の各エンドポイントは JAX-RS の Resource として実装されており、例えば /subjects/{subject}/versions であれば SubjectVersionsResource が相当する、などの構成になっています。スキーマなどの情報を永続化するストレージレイヤは KafkaStore クラスで提供されています。スキーマの登録のため put() が呼ばれると、スキーマ情報を ProducerRecord に包んで _schemas topic に 送ります。他方、スキーマの情報取得はというと KafkaStore 自体では直接は _schemas topic を consume せず、別スレッドで動作してローカルスキーマキャッシュを更新する KafkaStoreReaderThread が更新した InMemoryCache から取得する格好になります。この動作についてはドキュメントでも言及されています。docs.confluent.ioおわりにConfluent Schema Registry について筆者の思うまま気になるところをざっくり紹介しました。いかがでしたでしょうか。次回は Apache Pulsar の持つ Schema Registry 機能について調べて行こうかなと思います。なんとこちらのスキーマレジストリは Avro は勿論 Protocol Buffers もサポートしているようです！Protocol Buffers は各言語向けの自動生成されたライブラリを使う、 Avro で言う SpecificRecord のサブクラスに近い性質を持つ表現で扱われることが多いと思われますが、スキーマレジストリをどのように提供しているのでしょうか。pulsar.apache.org余談ですが本記事執筆中に Confluent Schema Registry でも Protocol Buffers をサポートする気配のある pull request を発見してしまいました。近い将来 Avro 以外でシリアライズしたレコードが色んなミドルウェアでらくらくに扱える日が来るかも知れませんね。github.com","link":"https://syucream.hatenablog.jp/entry/2020/01/31/012133","isoDate":"2020-01-30T16:21:33.000Z","dateMiliSeconds":1580401293000,"authorName":"syucream","authorId":"syucream"},{"title":"Apache Avro について知っていることを書いていく　その2","contentSnippet":"Apache Avro について書き下していく記事その 2 です。本記事では Avro で表現されるデータのプログラミング言語上の表現、特に Java を想定して SpecificData と GenericData について触れていきます。Avro のデータ型とコード生成前回の記事で触れた通り、 Avro では様々なリッチなデータ型を表現できます。多くの場合一番外側のスコープでは record type を使って構造体を定義することになるかなと思います。Avro スキーマは以下のように事前に JSON 文字列を埋め込むか .avsc ファイルで保存しておくのもアリですが、実行時に動的生成することもできます。{  \"name\": \"Hello\",  \"type\": \"record\",  \"fields\": [    {      \"name\": \"id\",      \"type\": \"long\"    },    {      \"name\": \"name\",      \"type\": \"string\"    }  ]}ここで定義したデータ型を実際にどう扱いましょうか？例えば Protocol Buffers ではちょうど Avro で .avsc ファイルに記述するように、以下のようにおおむね .proto ファイルにメッセージ型を定義していくことになると思います。syntax = \"proto3\";package com.syucream.example;message Hello {  uint64 id = 1;  string name = 2;}その後典型的には protoc と各言語に対応する protoc plugin を用いることで、定義したメッセージ型に従ったコードを生成することになります。$ protoc --java_out=./ hello.proto// Generated by the protocol buffer compiler.  DO NOT EDIT!// source: hello.protopackage com.syucream.example;public final class HelloOuterClass {  private HelloOuterClass() {}  public static void registerAllExtensions(      com.google.protobuf.ExtensionRegistryLite registry) {  }  public static void registerAllExtensions(      com.google.protobuf.ExtensionRegistry registry) {    registerAllExtensions(        (com.google.protobuf.ExtensionRegistryLite) registry);  }  public interface HelloOrBuilder extends      // @@protoc_insertion_point(interface_extends:com.syucream.example.Hello)      com.google.protobuf.MessageOrBuilder {    /**     * <code>uint64 id = 1;</code>     */    long getId();    /**     * <code>string name = 2;</code>     */    java.lang.String getName();   ...つまりアプリケーションに組み込んで用いる際に事前にコード生成を求められます。Avro ではコード生成はオプションです。事前に生成したコードを用いることも、そうでない選択肢を取ることもできます。基本的なデータ型表現セット GenericDataorg.apache.avro.generic.GenericData は Avro のデータ型の Java での表現の基礎となるクラスなどが詰まったユーティリティです。GenericData 以下に具体的な型に対応したクラスが存在します。例えば record なら org.apache.avro.generic.GenericData.Record といった具合です。以下では特にこの GenericData.Record を掘り下げましょう。まずこのクラスは org.apache.avro.generic.GenericRecord interface を実装しているので、まぁ名前の通りなのですがつまり値を get したり put したりできます。一見あまり情報量が無さそうなこの interface は、結構色々な Java 製ミドルウェアで頻出します。またこの get() の戻り値は Object 型であり、実際どんな型なのか意識するのは get() を呼び出すコードを書く人の責務になります。GenericData.Record オブジェクトを作るには、 Schema オブジェクトを渡して new します。Schema オブジェクトは Schema.Parser を使って事前定義した JSON 文字列をパースして得るのもよし、動的に Schema.createRecord() などから生成してもよし、です。この GenericData.Record ですが、フィールドの具体的な値は Object[] 型のメンバとして持ち、 get/put などをする時は Schema オブジェクトの情報からフィールドのインデックスを得てアクセスします。これはちょうど、 Avro の仕様としてシリアライズした後のバイナリのレイアウトがどうなっているかがスキーマによって決定づけられる（他にレイアウト情報を持たずスキーマを信じる）のに対応しそうです。GenericData を読み書きする GenericDatumReader/GenericDatumWriterorg.apache.avro.generic.GenericDatumReader を用いることでシリアライズされた状態のバイナリから GenericData 以下の Java のオブジェクトを取り出すことができます。ジェネリクスのパラメータは、例えば record 型の値を参照するだけでいいなら前述の GenericRecord を指定するので良いでしょう。逆に Java のオブジェクトをシリアライズしたい際は org.apache.avro.generic.GenericDatumWriter を使うことができます。より具体的な使い方は公式のテストコードを見ると参考になります。github.comAvro のシリアライズ方法とバイナリレイアウトを決めているのはスキーマです。というわけでもちろん、GenericDatumReader / GenericDatumWriter を用いる際にはスキーマ情報が求められます。さて実は GenericDatumReader には二種類のスキーマ、引数名でいうと reader と writer を与えることができます。名前の通り writer は GenericDatumWriter を使ってシリアライズする側で用いられた側のスキーマ、 reader は GenericDatumReader で読み出す側のスキーマです。具体的で実用しそうなシナリオとしては、 writer が古いスキーマでシリアライズする可能性のある環境下で reader としては新しいスキーマ（古いスキーマからの後方互換性がある）に従う型のオブジェクトとして扱いたい際に二種類のスキーマを与える場合がありそうです。より安全で高速な表現セット SpecificDataさてこの GenericData 関連のツールですが、値の取り回しは Object 型ですることになるので逐一キャストしなければならず、安全な実装をできるかやキャストするコストが発生する点が気になります。悪い言い方をすれば、シリアライズ・デシリアライズの操作を含めてスキーマに違反していないことをは保証された値でしかなく、 Java のコード上は例えば record 型なら単なる Map<String, Object> 型オブジェクトと言えそうです。幸いなことに Avro では GenericData を使うのとは別に SpecificData サブクラスを使う道も存在します。org.apache.avro.specific.SpecificData は GenericData のサブクラスです。と言ってもこのクラス自体にはさほど機能は追加されていません。重要なのはこれを継承した、自動生成された schema specific なクラスです。SpecificData なクラスのコード生成最初に述べた通り Avro には事前に .java ファイルのコードを生成しておく道もあります。コード生成は avro-tools を通して行えます。試しにこの記事の冒頭に出てきた .avsc からコード生成してみます。$ java -jar ~/tools/avro-tools-1.8.2.jar compile schema hello.avsc ./Input files to compile:  hello.avsclog4j:WARN No appenders could be found for logger (AvroVelocityLogChute).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.得られる Java のコードは以下のようになります（一部抜粋）/** * Autogenerated by Avro * * DO NOT EDIT DIRECTLY */import org.apache.avro.specific.SpecificData;import org.apache.avro.message.BinaryMessageEncoder;import org.apache.avro.message.BinaryMessageDecoder;import org.apache.avro.message.SchemaStore;@SuppressWarnings(\"all\")@org.apache.avro.specific.AvroGeneratedpublic class Hello extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {  private static final long serialVersionUID = -8266440640401408575L;  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"Hello\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"}]}\");  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }  ...  @Deprecated public long id;  @Deprecated public java.lang.CharSequence name;  /**   * Default constructor.  Note that this does not initialize fields   * to their default values from the schema.  If that is desired then   * one should use <code>newBuilder()</code>.   */  public Hello() {}  ...  /**   * Gets the value of the 'id' field.   * @return The value of the 'id' field.   */  public java.lang.Long getId() {    return id;  }  /**   * Sets the value of the 'id' field.   * @param value the value to set.   */  public void setId(java.lang.Long value) {    this.id = value;  }  /**   * Gets the value of the 'name' field.   * @return The value of the 'name' field.   */  public java.lang.CharSequence getName() {    return name;  }  /**   * Sets the value of the 'name' field.   * @param value the value to set.   */  public void setName(java.lang.CharSequence value) {    this.name = value;  }  ...嬉しいことにフィールドの型に対応した getter/setter が手に入ります！GenericRecord interface のメソッドを使うよりこちらの方が安全で若干高速になるはずです。見てきた通り SpecificData 系サブクラスはコード生成する都合コンパイル時にスキーマが決まっている必要があります。またスキーマ定義が頻繁に変更され、その度にコード生成し直しまくるのもやや煩雑かと思われます。あまり変わらない・変えたくないデータ型を定義する時は SpecificData からなるクラスを生成、動的に・頻繁に変わる可能性があるデータ型は GenericData として扱うのが良いかもしれません。Java 意外の言語ではどうなる？Java の例を見てきた通り Specific なテータ表現は Generic なもののサブセットとなっています。そんな訳で多くのプログラミング言語では Generic なデータ表現はサポートされています。が、 Specific によってはサポート状況がまちまちなようです。筆者が認識した限りでは、 C++ で experimental でサポート? されていたり、 Go では gogen-avro を作っている人がいます。飽きてきたので本記事はこのあたりで終わります。あとは筆者がよくハマる、複雑なスキーマを定義してみる話やシリアライズされた後のバイナリのレイアウトの話とか別記事で書くかもしれないです。","link":"https://syucream.hatenablog.jp/entry/2019/10/15/233158","isoDate":"2019-10-15T14:31:58.000Z","dateMiliSeconds":1571149918000,"authorName":"syucream","authorId":"syucream"},{"title":"Apache Avro について知っていることを書いていく　その1","contentSnippet":"Apache Avro になにかと縁があり、かつ普及しているテクノロジーの割に日本語の情報がそんなにない（個人の意見です、意外とあるかも）のでつらつら書いてみます。整理はされておらずシーケンシャルに要素を並べています。実装についてとくに言及がされていなければ、 Java の 1.8.2 について触れているものとします。Apache Avro はデータフォーマットとエコシステムのひとつApache Avro は Apache トップレベルプロジェクトのひとつでファイルフォーマットとその周辺エコシステムです。比較される技術として Protocol Buffers や Message Pack が挙げられると思います。Apache Avro はそれらの中でも主に Hadoop エコシステムなどビッグデータ絡みの文脈でよく登場するように思えます。以下のディレクトリ構成の通り、公式でいくつかのプログラミング言語をサポートしているようです。github.com筆者はほとんど Java のパッケージしか使わず正確な比較も行ってないのですが、おそらく Java 向けの機能がサポート厚めだと思われます。avro-tools が Java で書かれてたり avro-protobuf という Protocol Buffers との変換ライブラリが提供されていたりするので。また非公式ですが以下のような他言語向けライブラリもあったりします。github.comAvro はスキーマの表現力が強いAvro はスキーマに従いシリアライゼーションとでシリアライゼーションをするわけですが、このスキーマの表現力が割と強いです。雑に列挙すると以下の通りです。record 型(構造体)のサポートエイリアスや doc 、 namespace の宣言もできる(オプション)array 型(配列)のサポートunion 型(共用体)のサポートnull 型との union を取り、デフォルト値を null にすることで nullable な型を表現できるmap 型のサポートenum 型のサポートfixed 型という固定長バイト配列のサポート（使ったこと無いのでよくわからん）デフォルト値が設定できるlogical types を使うことで型に別の解釈をもたせることができるメジャーな使い方として long 型に timestamp-millis logical type をもたせるなどtimestamp の制度は micros も選べるこれらにより、かなりリッチなスキーマを記述することができるはずです。Avro のスキーマを記述するのがつらい表現力が高いこともあるのですが、基本的にスキーマは JSON で記述することになる上か結構冗長でかつ人間にとって読みにくい内容になりがちです。公式リポジトリに簡単な例があるのですが、簡単なものでもある程度事前知識を求められるでしょう。さらに型をネストし始めたら地獄です。筆者は特殊な訓練を繰り返すことにより最近手でスキーマを書けるようになってきた気がしますが、このスキーマの記載を同僚に求めるのは酷なものです。github.comこの問題はかなり認知されているのか、公式が IDL を提供しています。一見 Protocol Buffers を彷彿とさせる気がするシンタックスですね。avro.apache.orgまた、プログラムから動的にスキーマを記述、生成することもできます。Java であれば Schema.createRecord() して List<Schema.Field> オブジェクトを埋めて・・・といった流れで実現できます。Avro 自体の圧縮効率はそんな高くない？int, long は zigzag encoding してくれますが、 bytes, string は length + バイナリなどかなりシンプルなシリアライズをします。結果としてシリアライズされたあとのバイナリのサイズは Protocol Buffers や Message Pack と比較してそんなに潰れないと思います。（実測した訳でなく、単にこれまでの経験則程度の話です）ただ Object Container Files フォーマットを取るとブロック単位に圧縮をかけてくれたりします。Object Container Files フォーマットの便利さあなたがデータアナリストやデータサイエンティストなどのポジションで働いているなら、このフォーマットで Avro と対面することが多いかもしれません。.avro 拡張子をよく取る、 Object Container Files フォーマットに従うレイアウトを取ったファイルです。実体は well-known な Avro スキーマに従ってシリアライズされたファイルになります。そのスキーマの中にはレコードのスキーマとコーデック、フィンガープリント情報などが含まれます、ここのスキーマ情報により、ファイルを読むことで中のバイナリがどんなスキーマでシリアライズされたかわかるので動的にデシリアライザを生成して読める状態になってるわけですね！すぐれもの！このファイルフォーマットは BigQuery や Redshift でもサポートされており、 Hadoop エコシステムにあまり関わらずとも Avro のファイルをオブジェクトストレージに書き出しておけば、あとで DHW に統合して安全便利にクエリを投げられるわけですね！すぐれもの！avro-tools が開発運用に便利あまり言及されてない気がしますが、 avro-tools という Java 製のツールが便利です。事前定義した Avro のスキーマを記載した.avsc ファイルから Specific クラスを自動生成するのはもちろん、 Object Container Files フォーマットのファイルから schema を読んで吐いてくれたりレコードを json に変換して出力してくれたりします。特にレコードを json で出力してくれると、 jq などと組み合わせてフィルタしたり内容確認できたりして便利ですね。mvnrepository.com飽きてきたので本記事ではこれくらいにしておきます。気力があったり次回書きたいことがまとまってきたらまた書きます。","link":"https://syucream.hatenablog.jp/entry/2019/10/09/012545","isoDate":"2019-10-08T16:25:45.000Z","dateMiliSeconds":1570551945000,"authorName":"syucream","authorId":"syucream"},{"title":"「データとML周辺エンジニアリングを考える会」という勉強会の第二回を開催しました","contentSnippet":"TL;DR2019/07/19(金)に、ヤフー株式会社様コワーキングスペースの LODGE において、「データとML周辺エンジニアリングを考える会」という勉強会の第二回目を開催しました。data-engineering.connpass.comデータエンジニアリングとサイエンス、アナリティクスにざっくり被るような少し広く曖昧なドメインで開始した勉強会です。今回は前回より少々多めで、 40 人強の参加者が参加してくれました。発表、 LT に合わせて懇親会も議論が弾み、主催のひとりとしては実現したかったことがある程度できたのではないかと考えています。つらつら発表内容せっかくなので覚えているうちに発表資料のリンク記載と超個人的な感想載せてみます。15 分枠GCPでStreamなデータパイプライン運用しはじめた by @shoe116メルカリでのログ収集のためのパイプラインの構築の話です。マイクロサービスアーキテクチャへの移行やビジネス・組織のスケールに合わせた試行錯誤の跡が伺えます。というかわたしも業務で参加してるやつです。実際上記の試行錯誤をしています。speakerdeck.com行動ログ処理基盤の構築 by @hirosassaサービスにおける行動ログの収集基盤の刷新の話です。現行システムが pull 型で基盤側がサービスの内情を知ってしまう問題を、 push 型のアーキテクチャにしたあたりが今後の投資になりうるおもしろい点なのではと思います。（基盤システムってどこまでサービスのことを知るか、責任分界をどこでするかしばしば悩ましくなりますよね）speakerdeck.comLT 枠Google Cloud ML Engineに浸かってみる by @yudeayaseGCP の ML Engine の話です。ML Engine, 便利そうではあるもののこの仕組みに特化してしまうのは良いのか？などと考えさせられました。cloud.google.com（資料アップロードなし？あとで調べるPoC案件が多すぎてつらいので、パイプラインを使いまわすツールを入れた。 by @mori_kaz0429繰り返し発生する PoC 案件で、似たようなクエリを投げたりすることが多い処理を共通化、再利用可能にする話です。最後の方には Apache Airflow などワークフローエンジンを今後使ってみたい話も。（資料アップロードなし？あとで調べるCloud Composer & Cloud Dataflow によるバッチETLの再構築 by @yuzutas0Cloud Composer (Apache Airflow のマネージドサービス) を使って壊れかけのデータ同期の仕組みを立て直す話です。データエンジニアとデータアナリスト、両方に対してヒアリングをかけつつ現状を鑑みて良いバランスのところを攻めるというマネジメントに近い側面もあれば、 Cloud Dataflow によるクレンジング処理に触れたりする話もあり盛りだくさんでした。speakerdeck.comDigdagでETL処理をする by @nakano_shota今度は Digdag でワークフローを組んだ話です。s3_touch というオペレーターを開発して、 s3_wait と組み合わせてプロジェクト間依存関係も対応できるようにしていて良い。あまり深く触れられなかったけどリトライ処理や冪等性担保大事ですよね。もっとお話聞いてみたい気がします。speakerdeck.comComet.ml で AutoML ライブラリ開発（仮） by @Y_oHr_NComet.ml 、初めて知ったのですがかなり良さそう！codecov でカバレッジを可視化するのと同じようにモデルの可視化をできるのは好感触！speakerdeck.comデータ活用の際にハマってしまったログ・データスキーマ設計 by @yu-ya4この手の苦労話、これこそこういう勉強会で話し合いたかったことな気がします。テーブルの日付が何の時間を表すか問題、スキーマ更新にどう立ち向かうか、 STRING 型フィールドに JSON 突っ込むのどうなんだ話、 null の扱いと結構あるある話な気がするが・・・。二番目の話題は個人的にも刺さるものがありました。簡易にアプリケーションからログを出力して、読み出す時に苦労するスキーマオンリードの戦略自体は間違ってはいないはずだし・・・。触れられていなかった別の点として、 BigQuery はカラムナでデータを持ってくれているはずなのですが、 JSON を突っ込んで読む時にパースするではパフォーマンスが落ちる（課金が増える？）かもしれないなと思いました。speakerdeck.com今後に向けて幸いなことに参加者の方々からもそれなりに好評を得られたようだし、自分としても知見を得たい気持ちもあるため、主催メンバーで話し合いつつ第三回を企画していきたいと考えています。ジャストアイデアですが、初心者枠？というかこれからデータ基盤を作っていこうとする人たちが発表しやすい枠を設けるとかもあるとイベントの雰囲気変わるかなとか。その時が来たらまたアナウンスしますので、ご興味ある方々いらっしゃいましたらぜひぜひ！！","link":"https://syucream.hatenablog.jp/entry/2019/07/24/024314","isoDate":"2019-07-23T17:43:14.000Z","dateMiliSeconds":1563903794000,"authorName":"syucream","authorId":"syucream"},{"title":"技術書典5で配布した同人誌の原稿データを GitHub で公開しました","contentSnippet":"こちらで紹介した、僕が主催するサークルで技術書典5で配布した同人誌の原稿データを公開しました。syucream.hatenablog.jpリポジトリはこちらになります。github.com前回と同様、 Re:VIEW を使って記述しています。epub / pdf ファイルが欲しい方はいい感じにビルドしてください。もし多少でも気に入った方がいらっしゃれば、 Kindle 版を購入していただけると幸いです！僕のジュース代の足しくらいにはなります。www.amazon.co.jp","link":"https://syucream.hatenablog.jp/entry/2018/12/21/002509","isoDate":"2018-12-20T15:25:09.000Z","dateMiliSeconds":1545319509000,"authorName":"syucream","authorId":"syucream"},{"title":"Kubernetes と CSI(Container Storage Interface) について","contentSnippet":"この記事は Kubernetes2 Advent Calendar 4 日目の記事です。本記事は CSI(Container Storage Interface) と Kubernetes での CSI のサポートについて触れます。執筆に時間があまり割けなかった為、後でもう少し加筆する、あるいは別途続きの記事を書くかもしれません。CSI(Container Storage Interface) とはCSI(Container Storage Interface, 以下 CSI) は Kubernetes など複数の Container Orchestration (CO) から共通のプラグインを使って SP (Storage Provider) とやり取りするためのインタフェース仕様です。より具体的には、 CO からストレージを抽象化した Volume をアタッチ・デタッチしたりスナップショットを取ったりする機能のためのインタフェースを提供します。CSI は Kubernetes とは独立したプロジェクトで進行しております。github.comまた Kubernetes のプロジェクト以下で、 Kubernetes から SP が提供する CSI プラグインへ通信するための CSIDriver と関連実装を以下プロジェクトで進行しています。github.comKubertenes としては CSI は 1.9 で Alpha 、 1.10 で Beta サポート対象になっていますまた Kubernetes での CSI 利用については他に既に試されている方もいるようです。こちらの記事は詳細に踏み込んだ構成説明などもされており、オススメできる内容になっています。qiita.comCSI の仕様CSI のインタフェース仕様は Protocol Buffer で定義されており、以下リポジトリで管理されています。またこのリポジトリ内の lib/go/ ディレクトリには .proto ファイルの定義に従ったライブラリの Go 実装が配置されています。github.comCSI のプラグインは Controller plugin, Node plugin の二種類が想定されております。また CO とこれら CSI プラグイン間は gRPC で通信することが想定されています。CSI のサービスCSI のサービスとして以下三種類が想定されています。構成によっては実装しなくても良いサービスやメソッドがありますが、少なくとも Node Service の NodePublishVolume, NodeUnpublishVolume は実装する必要があります。（でないと Node から Volume を参照できないはず）Identity ServiceCO から Plugin のケーパビリティ (後述) やヘルスチェック、メタデータ参照を可能にするサービスController ServiceNode をまたいで Volume の構成を管理するためのサービスVolume のスナップショット操作もサポートするNode Service各 Node から Volume の操作を可能にするためのサービスCSI における Volume の状態遷移CSI で Node から Volume を利用されるまでに、 Volume は基本的に以下のような状態遷移をしていきます。幾つかの状態は、プラグインが示す後述の CSI ケーパビリティによって存在しなかったりします。   CreateVolume +------------+ DeleteVolume +------------->|  CREATED   +--------------+ |              +---+----+---+              | |       Controller |    | Controller       v+++         Publish |    | Unpublish       +++|X|          Volume |    | Volume          | |+-+             +---v----+---+             +-+                | NODE_READY |                +---+----^---+               Node |    | Node              Stage |    | Unstage             Volume |    | Volume                +---v----+---+                |  VOL_READY |                +------------+               Node |    | Node            Publish |    | Unpublish             Volume |    | Volume                +---v----+---+                | PUBLISHED  |                +------------+ref. https://github.com/container-storage-interface/spec/blob/master/spec.md#volume-lifecycleCSI のケーパビリティCSI プラグインでどのような操作をサポートするかの情報です。CSI の仕様としては以下 4 種類のケーパビリティをサポートします。PluginCapabilityCSI プラグイン全体でサポートする機能Controller Service を提供するか、オンラインボリューム拡張をサポートするかなどの情報を含むCO は GetPluginCapabilities メソッドで取得可能VolumeCapabilityVolume のファイルシステムのタイプやアクセス制御（リードオンリーなのかなど）、容量などの情報を含むCO は CreateVolume, ControllerPublishVolume などのメソッドで指定や値の検証が可能ControllerServiceCapabilityCreate/Delete , Publish/Unpublish などの操作をサポートするかの情報を含むCO は ControllerGetCapabilities メソッドで取得可能NodeServiceCapabilityStage/Unstage などの操作をサポートするかの情報を含むCO は NodeGetCapabilities メソッドで取得可能CSI の利用用途CSI プラグインとしてはすでに、 NFS や iSCSI などの公式のサンプルプラグインを含め、 AWS や GCP のストレージサービスの利用を可能とするプラグインが存在します。以下の公式ドキュメントに既に存在する CSI プラグインに関する記述があります。kubernetes-csi.github.io簡単な CSI Plugin の実装を追ってみる以上だけだと CSI プラグインの動作が把握し難いので、 kubernetes-csi の drivers リポジトリにある HostPath plugin の動作を追ってみます。このプラグインはリポジトリに kubernetes への deploy 用 yaml ファイルも同梱されており動作を追うのに便利です。HostPath plugin では CSI の Identity, Controller, Node 三種類の gRPC サービスを提供します。その実装は github.com/kubernetes-csi/drivers/tree/master/pkg/hostpath に存在します。Identity Server は特に特殊な実装をしていません。github.com/kubernetes-csi/drivers/blob/master/pkg/csi-common に存在する、他の Driver と共通のデフォルトメソッド実装をそのまま使用しています。Controller Server では CreateVolume, DeleteVolume , CreateSnapshot, DeleteSnapshot, ListSnapshots を実装します。CreateVolume, DeleteVolume では複雑なことをしておらず、 UUID 付きの名前のディレクトリを掘って Volume に見立て、 map で Volume の管理を行います。CreateSnapshot, DeleteSnapshot はそれに似て、 Volume のディレクトリを tar czf で固めて同じく UUID 付きのファイルに保存・管理します。Node Server では NodePublishVolume, NodeUnpublishVolume, NodeStageVolume , NodeUnstageVolume を実装します。NodePublishVolume, NodeUnpublishVolume は作成された Volume のディレクトリを k8s.io/kubernetes/pkg/util/mount パッケージを介して mount/unmount します。NodeStageVolume , NodeUnstageVolume は単にリクエストパラメータ中の Volume や ターゲットパスが空でないことをチェックしているだけになります。HostPath プラグインでは他に複雑なチェックをする必要が無いためこうなっているのではと思われます。kubernetes 上で想定される構成は以下の通りになっているようです。Controller Server のメソッドの参照CreateVolume など Volume の作成・削除は https://github.com/kubernetes-csi/external-provisioner コンテナを利用ControllerPublishVolume など Volume の Publish 操作は https://github.com/kubernetes-csi/external-attacher を利用CreateSnapshot などスナップショットの操作は https://github.com/kubernetes-csi/external-snapshotter を利用Node Server のメソッド参照https://github.com/kubernetes-csi/driver-registrar を使って CSIDriver を kubelet から参照可能にするkubelet から CSIDriver をプラグインにという格好で読み出し参照するロジックは Kubernetes 本体リポジトリの kubelet と関連パッケージに存在おわりに以上、 CSI とその Kubernetes へのインテグレーションに関する記事でした。CSI は可搬性を担保しつつ Kubernetes での利用を重要視されて作られているようですが、 CSI の仕様含めエコシステムもまだ枯れているとは言えず、機能も多くはありません。Kubernetes 用の単なるサンプルとはいえ HostPath プラグインで tar コマンドを実行してスナップショットを作成したりなど、もう少し抽象化されてほしい箇所もまだ存在します。しかしながら個人的には Kubernetes での柔軟なストレージ利用には、以前の記事で挙げた通りまだまだ課題があると思っており、今後の発展を期待したいと考えています。syucream.hatenablog.jp","link":"https://syucream.hatenablog.jp/entry/2018/12/04/004539","isoDate":"2018-12-03T15:45:39.000Z","dateMiliSeconds":1543851939000,"authorName":"syucream","authorId":"syucream"},{"title":"Kubernetes 上で動作するコンテナから安全に FUSE を利用したかった","contentSnippet":"本題の通りの気持ちがあったのですが、結論としては手軽にできる良い方法は無いようでした。備忘録的に挑戦した事を記録しておきます。背景: FUSE 利用のモチベーション言うまでもなくファイル I/O はシステム開発においてよく使われる機構であり、多くのプログラミング言語において標準ライブラリまたはそれに類するライブラリでファイル I/O をサポートしていると思われます。さて Kubernetes 上でマイクロサービスを実装していく事が多々あるこのご時世、各マイクロサービスで実装言語非依存で使えるデータの読み書きのロジックがあると、例えば何らかの設定ファイルの動的受け渡しやロギングにおいて便利な可能性があります。これに似た課題を抱えて解決に向かったプロダクトとして Envoy Proxy があるかと思われます。これは元々ネットワーキングやトレーシングの課題などを各言語ごとのライブラリで対応していたもののつらくなり、 HTTP や gRPC を解釈するプロキシを導入することで緩和に成功しています。前述の通りファイル I/O は HTTP や gRPC をしゃべるより更にリーズナブルな共通プロトコルであると考えられもしそうです。さらに FUSE を使えばファイルシステムとしてインタフェースを提供して、かつ裏側で複雑なロジックを動かすこともできるように考えられます。そんなわけで、 Kubernetes 上で動作するコンテナからいい感じに FUSE で実装したファイルシステムをマウントして利用できると良いかもと着想した次第です。FUSE 利用の限界同じような思考をしたり、あるいは既存 FUSE 資産を利用したい人たちが多々居たようで、 Kubernetes 公式リポジトリの Issue でも議論がされていたりします。github.com残念ながらこの 2015 年に open した issue は 2018 年も終わりを迎える今日においても close されていません。どういう点がネックになるかというと/dev/fuse を open するのに特権が必要mount, umount するのに SYS_ADMIN ケーパビリティが必要が挙げられ、一応利用できなくは無いものの利用条件を満たすため現状は privileged mode でコンテナを動かす羽目になり、やや安全面でリスクがあるように思えます。FUSE 利用パターンいくつか前述の Issue の中で幾つかのアイデアも提示されています。少しこれらを実際に試してみることにしました。多少このリスクを緩和するため、大きく分けて以下の方針がありそうです。コンテナ外でなんとかするパターン特権を持つコンテナをアプリケーションを動作させるコンテナと分離するパターン前者は場合によっては可搬性を損なうか導入が非常に困難になる恐れがあり、また後者であればコンテナ内の世界で完結させることができそうです。従ってまずは後者のアイデアを試してみます。以下に検証用に用意した Dockerfile や設定ファイルを配置しました。FUSE でマウントするのは libfuse に含まれる、ファイルを read すると \"Hello, World!\" を返す hello.c を利用しておきます。github.comこちらのリポジトリでは initContainers で mount するのと、 postStart , preStop でサイドカーコンテナで mount/umount する案を試してみました。が、結局マウント先を参照するアプリケーションコンテナからはマウントした先は参照できなくなります。結局この例だと特権を渡さざるを得ませんでした。次の一手のアイデア今回特権を要求されたのが、コンテナ内から /dev/fuse が open できない点なので、この点を何とかできれば 多少課題は緩和されるかもです。Kubernetes は Device Plugin という機構を最近サポートしているようで、これで何とかできる可能性があるような全く無いような気がしています。また Kubernetes v1.10 からベータになったというストレージ接続インタフェース CSI は将来も見据えてこの問題に取り組むのに、もしかしたら良い機構かもしれません。いずれにせよ Kubernetes 自体の調査をほとんどやっていないので次の一手はこれら周辺技術の調査からかと考えております。","link":"https://syucream.hatenablog.jp/entry/2018/11/05/011908","isoDate":"2018-11-04T16:19:08.000Z","dateMiliSeconds":1541348348000,"authorName":"syucream","authorId":"syucream"},{"title":"ISUCON8 に参加して最終成績が本戦3位だった","contentSnippet":"タイトルのとおりです。何やかんやあり ISUCON8 の予選を無事突破した後に 10/20(土) LINE さんのオフィスにて本戦に参加して、最終成績 3 位に収まりました。3 位だと特に表彰されるわけでもなく気持ちのみなのですが。チーム構成と役割分担について職場のよしみで cubicdaiya さん、  catatsuy さんと何らか SRE チームらしき雰囲気を醸し出しながら参戦しました。大枠としては以下の役割分担を組んでいました。インフラ担当cubicdaiyaアプリ担当catatsuysyucream全体チーム構成やツール・環境構築お膳立ては catatsuy さんがいい感じにやってくれていました。予選について本記事執筆にあたり予選実施日と期間が空いて、ほぼすべての記憶を損失しました。様子はたぶん catatsuy さんの以下記事にある気がします。medium.com本戦についてだいたい出題された課題の要点は以下記事で説明されています。isucon.net3 位に入賞できた決めては突出したコレという点は無く、チームでそれぞれ貢献できた、バランス的には良い塩梅なのではと感じました。具体的なタイムラインとしては覚えている限り以下のとおりです。前半: 好調な滑り出しとりあえず各々初期準備をする自分はここでは DB スキーマを取り出して共有したり WebUI 調べたりcatatsuy さんが早期にログ分析APIの叩き方問題に気づく着想としてはrate limitとかより重要なパスでI/Oしていそうなことがやばそう11 時台に議論して、ハマりそうだから手を打とうと結論づけて catatsuy さんが着手しだすsyucream が LIMIT 1 問題に気づく最初はサブクエリ書く感じで改善したが LIMIT 1 するのと課題的に差分がそれほどなかったここで一時的に 5000 点ほどスコアを叩き出しトップに躍り出る時間軸が入れ替わるが、この改善の実施直前に cubicdaiya さんが slow query を出してくれていてプロファイルの裏付けもした上で実施したcubicdaiya さんがインフラ周り、特に nginx 周辺のチューニングを徐々にする冗長そうなコンテナ化をやめる静的ファイルを nginx で配信中盤: 次ステップへの到達との葛藤会場提供でランチとして弁当が出るAbemaTV のニュースで鯖の漁獲量が減って値段が若干上がった話を思い出し、僕は思わず鯖味噌煮弁当を確保catatsuy さんのログの集約送信が動き出すsend_bulk API の利用がうまくいかず、時間ももったいないのでバッファリングして send するだけにとどめたりこの瞬間はスコアに響かなかったが、後々ボディブローのように効いてきた感じがあるsyucream が地味に無駄そうな user table のレコードロックを外すslow query ではあったが、全くスコアに響かなかったcubicdaiya さんがそろそろいいでしょと良い SNS share 機能を有効化するsyucream が雑にロウソク足チャートのオンメモリキャッシュを入れるマージした後に、一度キャッシュをすると別タイミングに対してのクエリにもキャッシュヒットしてしまうバグに気づくが、ベンチマーカが文句を言わなかった！一瞬チームで相談して、問題が出てないなら後で考えようと結論づけるcubicdaiya さんが複数台構成を考え始める特に異論無く、 1: DB, 2: App, 1: nginx という意思決定が即なされる後半: 着眼点は悪くなかったがもう一歩だったかcatatsuy & syucream で orders & trade の課題について考え出すかなり真面目に巨大トランザクションを引き剥がすとか検討したが、いまいち鳴かず飛ばずその裏で cubicdaiya さんが App サーバを 2 台構成にしていたこのあたり？でスコアが 10000 近くに到達catatsuy さんがミクロな最適化したり、 syucream が銀行 API アクセスを並列化したり N+1 を無くそうとして爆死したりその裏で cubicdaiya さんが 4 台フルに使い切る構成への移行を追えてチューニングに入ったりDB の負荷をさげたくて ORDER BY を使うクエリを Go のアプリレベルに移植して時間がたりなかったり一部さばけない負荷に対して cubicdaiya さんが async sleep 入れたりsyucream はこれが大きな差別化ポイントだった気がしているそうこうしている間に 17:50 頃になって収束させて終了'もう一歩' に対する所感着眼点は非常に良くて、 PARTITION に対する疑惑や POST /orders 、 RunTrade() 最適化は課題に感じていて一部取り組んでいましたあらかじめ配布された spec を読み込んだり、外部 API に対して向ける疑惑がもっとあっても良かったかも結論: ISUCON はいいぞ今回の出題の外部 API が絡む話は個人的によく練られていて、個人的に非常に好ましい話でした。なぜならこういう外部APIとその挙動の差異は現実問題に起こりうりそうな話だからです。そういう観点から言うと、学生チームが優勝したという展開はとても興味深いようにも思えます。個人的課題として、自分が当日見ていた箇所で見どころは誤っていないけど一歩足りなかったことが挙げられ、狂おしいほど悔しいです。もし次回参加して同じメンバーでチームを組むのなら、 catatsuy さんが優勝するのに並々ならぬ熱意を抱いているのでたぶん僕らが優勝するでしょう。","link":"https://syucream.hatenablog.jp/entry/2018/10/25/030930","isoDate":"2018-10-24T18:09:30.000Z","dateMiliSeconds":1540404570000,"authorName":"syucream","authorId":"syucream"},{"title":"POSIX message queue を Go のコードから利用するためのライブラリ posix_mq を作った","contentSnippet":"表題の通りです。github.comcgo を使って POSIX message queue の基本的な操作、 open/close と send/receive とその他細々とした機能を実装しています。とは言っても、それほど複雑なことはしておらず、 POSIX の関数呼び出しを愚直に Go の func にラップしているだけなのですけどね。なぜやったのか入門 Kubernetes などを読むと、 Pod 内のコンテナ同士では SysV / POSIX の IPC namespace を共有している記述があります。www.oreilly.co.jpPod 内の別コンテナへの通信となると、 sidecar パターンで Envoy を動かすようなネットワーキング用プロキシを介するのに利用したり fluentd などのロギングエージェントにログを送ったり、お決まりのパターンがあると思います。そういった際に低コストで非言語依存なプロトコルが欲しくなることが多々あるように考えられます。POSIX message queue は POSIX としての仕様も存在し、低コストな IPC 手段のひとつです。これを選択肢のひとつとして用意しておくことは発生する課題に柔軟に対応するのに重要であると考えます。手段は他にもあるし、同様の機能であれば SySV message queue やいっそ AMQP などをしゃべっても良い気もするのですが、手段を増やす意味でも今回のライブラリを開発してみた次第です。動作例sender / receiver の通信例シンプルなsender と receiver であれば以下のコードで実装してやり取りできます。sender.gopackage mainimport (    \"fmt\"    \"log\"    \"time\"    \"github.com/syucream/posix_mq/src/posix_mq\")const maxTickNum = 10func main() {    oflag := posix_mq.O_WRONLY | posix_mq.O_CREAT    mq, err := posix_mq.NewMessageQueue(\"/posix_mq_example\", oflag, 0666, nil)    if err != nil {        log.Fatal(err)    }    defer mq.Close()    count := 0    for {        count++        mq.Send([]byte(fmt.Sprintf(\"Hello, World : %d\\n\", count)), 0)        fmt.Println(\"Sent a new message\")        if count >= maxTickNum {            break        }        time.Sleep(1 * time.Second)    }}receiver.gopackage mainimport (    \"fmt\"    \"log\"    \"github.com/syucream/posix_mq/src/posix_mq\")const maxTickNum = 10func main() {    oflag := posix_mq.O_RDONLY    mq, err := posix_mq.NewMessageQueue(\"/posix_mq_example\", oflag, 0666, nil)    if err != nil {        log.Fatal(err)    }    defer mq.Close()    fmt.Println(\"Start receiving messages\")    count := 0    for {        count++        msg, _, err := mq.Receive()        if err != nil {            log.Fatal(err)        }        fmt.Printf(string(msg))        if count >= maxTickNum {            break        }    }}Kubernetes の同一 Pod 上 container 通信例折角なので上記の sender / receiver を Kubernetes の Pod に押し込んで通信させてみます。あらかじめてきとうに sender / receiver 用の Docker イメージを作っておいてください。Pod の定義なのですが、愚直に container の設定を羅列していくだけです。IPC namespace は勝手に共有されるのでそれに関する設定や準備は必要ありません。apiVersion: v1kind: Podmetadata:  name: posixmq-podspec:  containers:    - name: posixmq-sender      image: \"posix_mq_sender\"      imagePullPolicy: IfNotPresent    - name: posixmq-receiver      image: \"posix_mq_receiver\"      imagePullPolicy: IfNotPresent  restartPolicy: NeverPod の動作確認をさくっとしてみましょう。$ kubectl apply -f example/kubernetes/pod-posixmq.yamlpod \"posixmq-pod\" created...$ kubectl logs posixmq-pod -c posixmq-sendergo run example/exec/sender.goSent a new messageSent a new messageSent a new messageSent a new messageSent a new messageSent a new messageSent a new messageSent a new messageSent a new messageSent a new message$ kubectl logs posixmq-pod -c posixmq-receivergo run example/exec/receiver.goStart receiving messagesHello, World : 1Hello, World : 2Hello, World : 3Hello, World : 4Hello, World : 5Hello, World : 6Hello, World : 7Hello, World : 8Hello, World : 9Hello, World : 10この出力結果を見るに、 sender の送ったメッセージがちゃんと receiver に届いていそうです！余談POSIX の機能となると可搬性を期待してしまいますが、 POSIX message queue は darwin や windows では実装されていなかったりと意外に可搬性に欠けます。対して SysV の message queue はこれに比べて可搬性が高く、より多くの環境でサポートされています。（このあたりは Linuxプログラミングインタフェース にも記述されていますね！）www.oreilly.co.jpとは言っても本記事で書くようにあらかじめ環境が定められている Kubernetes クラスタ上で動かす場合は、それほど気にすることでも無いのかもしれません。また、 SysV message queue の Go ラッパーライブラリは Shopify により実装されているのでこれを試すのもアリかもです。github.comちなみに少し前のベンチマーク内容ですが、 POSIX message queue は IPC の手段として結構高パフォーマンスであるような調査結果もあります。www.programering.com","link":"https://syucream.hatenablog.jp/entry/2018/10/15/001313","isoDate":"2018-10-14T15:13:13.000Z","dateMiliSeconds":1539529993000,"authorName":"syucream","authorId":"syucream"},{"title":"技術書典5にてマイクロサービスとEnvoy、暗号通貨についての薄い本を配布します","contentSnippet":"明日 10/08 (月) は技術書典5 の日ですね！techbookfest.org当サークル「まいにちがきんようび。」もサークル参加して、新刊を配布する予定です！（既刊の配布予定はありません）techbookfest.org内容としては、マイクロサービスと Envoy Proxy を試してみた結果からの紹介記事をメインに、前回記事執筆者による bitcoin の仕様の闇の記事を付録に添えたものになります。具体的には以下のような内容になります（目次から抜粋）第1章　Envoy Proxy 入門1.1　はじめに1.2　マイクロサービスアーキテクチャ概要1.2.1 モノリシックアーキテクチャ1.2.2 マイクロサービスアーキテクチャ1.3　マイクロサービス、そして Envoy と Istio1.3.1 Envoy とは1.3.2 Istio とは1.4　Envoy 詳解1.4.1 Envoy アーキテクチャ概要1.4.2 Envoy のリソース抽象化1.4.3 Envoy の特徴的な機能説明1.4.4 nginx など従来のプロキシと何が違うのかについて1.5　Envoy の試し方1.5.1 Docker image をとりあえず動かす1.5.2 複雑な構成を試してみる1.6　おまけ1.6.1 Istio における Envoy の組み込まれ方1.6.2 Envoy ソースコードリーティング1.7　まとめ付録A　私が暗号通貨を嫌いになったわけA.1　はじめにA.2　前提知識A.2.1 トランザクションの構造と所有権の移転履歴A.2.2 TXID とトランザクションデータA.2.3 コインベーストランザクションとマイニング報酬A.2.4 マークルツリーとマークルルートA.2.5 マイニングと NonceA.2.6 コインベーストランザクションとエクストラ NonceA.2.7 TXID の衝突確率と鳩ノ巣原理とバースデイパラドックスA.3　TXID の衝突事例と BIP-30A.4　BIP-30 から BIP-34 へA.5　BIP-34 以前のトランザクションとの衝突問題A.6　BIP-30,34 と各種アルトコインA.7　実際の衝突発生確率A.8　おわりにあとがきぜひ会場でお目に止まるようであれば、手にとってみていただけると幸いです！また当日会場に来ない、来れない、あるいは僕にとっては運良く完売してしまって購入できなくなったという時のために kindle 版も用意しております。こちらも合わせてご検討いただければ幸いです。https://www.amazon.co.jp/dp/B07HYC1HLN","link":"https://syucream.hatenablog.jp/entry/2018/10/07/212525","isoDate":"2018-10-07T12:25:25.000Z","dateMiliSeconds":1538915125000,"authorName":"syucream","authorId":"syucream"},{"title":"Netflix のデータパイプラインを読み解きたい","contentSnippet":"Netflix はマイクロサービスアーキテクチャ界においてプロダクションで成功例を積んでいる、いわば大先輩だと思われます。彼らは数多くのイベント登壇や techblog の記事、 GitHub 上による OSS の公開を行っており、それらからアーキテクチャやその変遷を垣間見ることができると考えています。本記事では筆者が最近悩んでいる、マイクロサービス前提の世界でのログ収集基盤において、 Netflix の様々な事例を調べた結果をつらつら書いていこうと思います。あらかじめ本記事は正確性を担保しておらず、あくまで筆者個人が調べることができた範囲での記述に留まることをお断りさせていただきます。Suro: 分散データパイプライン2015 年くらいにメンテが止まってしまったのですが、分散データパイプラインをうたう Suro というソフトウェアが存在しました。Suro に関しては解説記事も書かれていて、イベントログを集約し後続の S3 、 Kafka 、その他イベント処理システムに転送する役割を担っていたようです。またバッチとリアルタイム、両方の性質を持つデータを受け付けていたようです。medium.comSuro がメンテされなくなった経緯は筆者もよく分かっていませんが、後続のデータパイプラインの仕組みを見るにバッチとリアルタイムの両方の要望に答えるのが辛かったのか、あるいはデータの分析基盤の変更に合わせて作り直す決定をしたものかと推測しています。Cassandra のデータ転送のバッチレイヤーと Ursula によるスピードレイヤーSuro と入れ替える形かどうそうでないかは定かで無いですが、以下の資料によると Lambda Architecture に似たバッチレイヤーとスピードレイヤーの分離を行っているのが見て取れます。https://qconsf.com/sf2016/system/files/presentation-slides/netflix-cloud_analytics.pdfバッチレイヤーでは、 Netflix で広く使われている Cassandra をベースに SSTable をダンプして、それを aegisthus という Cassandra 向けバルクデータパイプラインソフトウェアを使って、 Netflix がデータウェアハウスに使っている S3 に転送していたようです。また SSTable のダンプは上記資料では明示されていないのですが、 Priam というツールが公開されておりこれを使っているのではないかと推測しています。ちなみにこの aegisthus も、 2017 年にメンテナンスモードに移行し今ではコミットされていないようです。最後のコミットに対してされたコメント を見るに、 Cassandra のデータのデータウェアハウスへの転送は今後 Spark のジョブに移行することを検討しており、 2018 年 8 月時点ではまだ移行途中？でかつ新しいツールは OSS にはなっていないようです。スピードレイヤーを支える Ursula というソフトウェアの存在も上記スライドでは触れられていますが、こちらは OSS になっている気配はありませんでした。データパイプライン周辺技術パイプライン以外にも、 Netflix はビッグデータ処理ジョブのオーケストレーションツールである genie や、 メタデータ管理ソフトウェアである metacat など多くの小道具を揃えているようです。余談上記のうち Suro や aegisthus については、 \"マイクロサービスアーキテクチャ\" にも記述があり参考になる部分があるかも知れません。www.oreilly.co.jpここで挙げたツールは Netflix が Cassandra を広く採用していることや、大規模なデータを持つがゆえの様々な課題、背景などもあって作ったのだと考えられます。これらがそのままマッチして利用できる企業は多いわけではないと思われますが、これらの変遷や構成などが読み解けるものがあるのかも、などと思いました（小並感）追記今は Kafka を結構活用しているのかも。ちょっと前の記事だけど、調べ損ねてた netflix のデータパイプライン、投入部分は Java library (+ Non-java app 向けの proxy) でやってるのかなー？  https://t.co/eCKkKf58KT— しゅー　くりーむ (@syu_cream) 2018年8月22日2017 年の資料にも sharing jar の話出てくるし投入部分は今もあまり変わってないのかしら https://t.co/oTIeDhLJY2— しゅー　くりーむ (@syu_cream) 2018年8月22日","link":"https://syucream.hatenablog.jp/entry/2018/08/21/235945","isoDate":"2018-08-21T14:59:45.000Z","dateMiliSeconds":1534863585000,"authorName":"syucream","authorId":"syucream"},{"title":"株式会社メルカリに入社して1年が経過した","contentSnippet":"ちょうど一年前の 2017年8月16日に株式会社メルカリに入社しました。キリがいいこともあり、ここらで個人的な振り返りをつらつら書いてみます。あらかじめ、はっきりいって個人の日記レベルの内容であることをお断りさせていただきます。転職の経緯から入社まで　2017 年 1 月頃から転職を考え初めていました。元の所属も Web サービスをなりわいとしており、それなりの愛着もあり良い経験をさせて頂いてはいたのですが、自分の能力向上を目指しつつより早いペースでプロダクトを提供していく体験を積みたく粛々と準備をしていました。そんな中メルカリは、急成長しているサービスを提供しており外部への発信も多く、また所属エンジニアも強力な方々が多く自分にとっても良い経験になると踏んだため応募した次第でした。　応募に際して僕の場合は特に知り合いのツテやエージェントを通さずに、採用ページの募集フォームにダイレクトアタックを仕掛けて選考に進みました。敢えて反省点をひねり出すならば、何とか所属するエンジニアにアプローチを仕掛けて雰囲気などを確かめるとより良かった気はします。採用ページや各種アウトプットからは見えない現場での葛藤やそこに根ざす文化を確認するのは、重要なことでしょうし。入社後から今まで　2017 年内はメルカリ SRE チームに所属して、ミドルウェア開発マンとして主にデータ収集基盤とその他細々としたコンポーネントの開発に携わっていました。その後は 2018 年 2 月頃？（うろ覚え）に子会社メルペイに籍を移し、一部マイクロサービスの開発や、メルペイのためのデータ収集基盤を、マイクロサービスアーキテクチャに従った世界でのプロダクトでの課題も吸収しつつ考えるという仕事に従事し始めました。　この間に起こった大きな変化として、個人的には前職が基本オンプレ前提のインフラ構成であったところからクラウドのリソースを多用する環境になったことと、会社的にはマイクロサービス化へ舵を切ったことです。特に後者は今でも悩ましい事がいくつもあり、刺激の多い日々を過ごせているように感じています。反省点と今後の目標　ここ一年多くの刺激を受けつつ過ごせましたが、まだまだ自分が納得できるほど大きな成功に到達していないとも思えます。元々、不確定な世界の中こつこつと少しずつ成果を積み上げ模索するのが好きであり得意であると考えているのですが、そのスタイルでは到達しにくい領域についても考えるべきかもという危機感も持ってきています。そういった現状の自身の不足しているスキルやできる領域、やれる領域を明確にしていくのが今後の課題なのかなぁ、などと考えています。おまけ: 仕事以外のここ一年の活動ngx_mruby のノンブロッキング sleep　メルカリ SRE チームに所属した際、そういえばちゃんと nginx を理解していないなと危機感を覚えつつ、勉強も兼ねて mruby スクリプトを動かすためのモジュール ngx_mruby にノンブロッキング sleep する仕組みを追加するパッチを書いてみていました。このパッチは結局幾つかの問題にハマり、自分だけでは解決できず半ば放置していたのですが、 matsumotory さんはじめ GMO ペパボさんの方々に拾っていただき、 RubyKaigi 2018 で紹介されるまでに至りました。・・・諦めて単純に放置するでなく、もう少し能動的に相談しに行ったりとかすべきだったかも、など様々な反省があります。rubykaigi.orgGo でいくつかツールを書いた　すでに記事と化したものもあるのですが、 Go でいくつかツールを書きました。syucream.hatenablog.jpsyucream.hatenablog.jp自分の業務に直結するスキルが得られる以外にも、やっぱ何も考えなくてもシングルバイナリで使えるとツールとしては便利だと考えたため、ひたすら Go で書いてます。で、誰？","link":"https://syucream.hatenablog.jp/entry/2018/08/16/122548","isoDate":"2018-08-16T03:25:48.000Z","dateMiliSeconds":1534389948000,"authorName":"syucream","authorId":"syucream"},{"title":"Cloud Spanner の DDL parser と DDL 変換ツールを作った","contentSnippet":"Cloud Spanner の DDL parser の Go 実装と、 Spanner DDL を MySQL DDL っぽいものに変換するツールを作りました。本記事はこれらの紹介になります。github.comgithub.comspar: Cloud Spanner DDL parser in GoSpanner の DDL は こんな感じ で、 SQL 方言に見えるもののカラムの型だの INTERLEAVE だの、仕様に対して独自の文法も結構な割合で持っているように見えます。今回作った spar, DDL parser はこれらをパースして CREATE 文や ALTER 文などの構成要素をまとめて返してくれます。同リポジトリに同梱した DDL syntax checker のコードがシンプルな使用例としても有用です。parser package の持つ Parse() に読みたい DDL の Reader を渡すとパースした結果を返してくれます。package mainimport (    \"io/ioutil\"    \"log\"    \"os\"    \"strings\"    \"github.com/syucream/spar/src/parser\")func main() {    data, err := ioutil.ReadAll(os.Stdin)    if err != nil {        log.Fatal(err)    }    _, err = parser.Parse(strings.NewReader(string(data)))    if err != nil {        log.Fatal(err)    }}パーサー部分は goyacc を使って生成しています。また lexer 部分の一部は https://github.com/benbjohnson/sql-parser を参考にしています。jackup: Jack up your DDL and translate between MySQL and Spannerスパナときたら今度は別の工具も欲しくなってくると思われます。ということでジャッキを作りました。jackup は標準入力か -f オプションで渡したパスのファイルの DDL を参照し、 spar を使ってパースした後、（現在は CREATE TABLE , CREATE INDEX しか読んでくれませんが） MySQL の同じような構造を持つ DDL を出力してくれます。残念ながら一部の変換、特にカラムの長さに関する制限が Spanner のほうがゆるい部分があり、無理やり変換したり変換せず無視したりもしています。このツールは Spanner を捨てて MySQL に以降したりなどクリティカルなユースケースに対応するつもりはなく、もう少しゆるく変換したらどうなりそうか確認したり、 MySQL 周りのエコシステム、例えば MySQL Workbench による ER 図生成に食わせたりしてみるために作ってます。なお、このあたりの SQL 方言とその間の相互変換は、現状良いソリューションが無く今回自分は自前で実装したのですが、今後は Apache Calcite に淡い期待を寄せてみてもいいのかなと思っています。Apache Calcite • Dynamic data management framework","link":"https://syucream.hatenablog.jp/entry/2018/08/08/004257","isoDate":"2018-08-07T15:42:57.000Z","dateMiliSeconds":1533656577000,"authorName":"syucream","authorId":"syucream"},{"title":"技術書典4で配布した同人誌の原稿データを GitHub で公開しました","contentSnippet":"以下の記事でご紹介した、弊サークルの技術書典4の新刊を GitHub で public repo として公開しました。syucream.hatenablog.jpリポジトリはこちらになります。github.comビルド済み pdf は無く、またその他配布した原稿にしか含まれないコンテンツなどあるかもしれませんが、ご了承ください。技術書典4振り返りいい機会なので振り返りをつらつら書きます執筆環境についてRe:VIEW で原稿を記述CircleCI で textlint 実行と pdf 自動ビルドするようにした入稿直前になって pdf がコレジャナイ感出るのマジ勘弁！なのでbase の Docker Image に vvakame/review を使わせていただきました細かなレビューは GitHub の pullreq or Issue ベースで実施進捗管理についてGitHub Issues の Milestone で、いくつかマイルストーンとその締切を設定ゆるめの締切にした最初から早割入稿前提で管理してた。追い込み記事にこのバッファがあることで少し精神的に助かったあまり予定通りにいかなかったのは大きな反省点多人数での執筆管理の経験が今回で初めてだったので、学びが多かった本のテーマについてテーマを 1 つに絞るか、開き直って雑誌形式にすべきだったテーマが 2 つあるせいで、そのテーマ間に関連があるものだと誤解させてしまったその他メンバーのモチベーションが途中で低減してしまったモチベ管理まですべきかは悩みどころ。同人誌であるなら、個々人が自分の記事の品質に納得が行っているならまぁいいのかな？表紙絵の担当が決まったのが後半だった早めに調整！！そして技術書典5へ〜弊サークル「まいにちがきんようび。」は技術書典5にも参加します！たぶんマイクロサービス周辺技術について執筆陣が好きな要素をひろってまとめた新刊を出すことになるかと思います（マイクロサービス関連以外にも、オマケとして記事を書くかもです）引き続きよろしくお願いいたします！！","link":"https://syucream.hatenablog.jp/entry/2018/08/06/225711","isoDate":"2018-08-06T13:57:11.000Z","dateMiliSeconds":1533563831000,"authorName":"syucream","authorId":"syucream"},{"title":"embulk-input-datastore を作った","contentSnippet":"TL;DRCloud Datastore からデータをぶっこ抜いてくるための embulk input plugin を作りました。github.com普通に embulk gem install embulk-input-datastore した上で input plugin としてご利用ください。参照するエンティティ、プロパティを絞り込むために GQL を指定可能にしているのでご利用ください。（逆にそれ以外の参照方法をサポートしていません。。）in:  type: datastore  project_id: \"your-gcppj-123\"  json_keyfile: credential.json  gql: \"SELECT * FROM myKind\"...細かい話実装言語Kotlin で実装しています。その理由としては Java よりシンプルに記述可能で、かつそれなりに事例があるので採用してみた次第です。qiita.comテストは Spek を用いて RSpec 的なシンタックスでテストコードを記述し、アサーションライブラリとして kotlin-test を使うようにしました。大した記述量では無いのですが、まぁまぁ様になっている気がします。embulk の input plugin としての挙動Cloud Datastore はいわゆる NoSQL データベースでありスキーマが固定されているわけではありません。従ってエンティティによって同じプロパティ名で違う型のプロパティが存在したり、値がそもそも存在しなかったりします。これらを embulk の（というか MessagePack の？）型にマッピングするのが面倒くさく、あまり良い対応方法も思いつかなかったので embulk-input-mongodb と同様単一の JSON フィールドに結果をまとめて出力するようにしました。input plugin として値の読み出しは行うが、変換処理を行う場合は別の filter plugin などでなんとかせい、というスタンスです。gradle-embulk-plugin の利用gradle-embulk-plugin を便利に使わせていただきました！主に初期のテンプレートコード生成や gradle embulk_run による動作確認などでお世話になりました。これから gradle でビルドしつつ embulk plugin 作るような方にはオススメしたいところです。動作例一応載っけてみます。以下のような myKind カインドに対するエンティティたちが存在する場合にこんな感じの設定ファイルを用意してin:  type: datastore  project_id: \"<my-project-id>\"  json_keyfile: credential.json  gql: \"SELECT * FROM myKind WHERE myProp >= 100 AND myProp < 200\"out:  type: stdout実行することで以下のような出力が得られました。$ embulk run examples/datastore2stdout.yaml2018-06-26 00:11:05.173 +0900: Embulk v0.9.72018-06-26 00:11:07.219 +0900 [WARN] (main): DEPRECATION: JRuby org.jruby.embed.ScriptingContainer is directly injected.2018-06-26 00:11:14.783 +0900 [INFO] (main): Gem's home and path are set by default: \"/Users/ryo/.embulk/lib/gems\"2018-06-26 00:11:17.364 +0900 [INFO] (main): Started Embulk v0.9.72018-06-26 00:11:17.639 +0900 [INFO] (0001:transaction): Loaded plugin embulk-input-datastore (0.1.1)2018-06-26 00:11:17.743 +0900 [INFO] (0001:transaction): Using local thread executor with max_threads=8 / output tasks 4 = input tasks 1 * 42018-06-26 00:11:17.784 +0900 [INFO] (0001:transaction): {done:  0 / 1, running: 0}{\"extra\":{\"defaultProperty\":\"hogefuga\"}, \"myProp\":100}{\"myProp\":150}2018-06-26 00:11:19.708 +0900 [INFO] (0001:transaction): {done:  1 / 1, running: 0}2018-06-26 00:11:19.714 +0900 [INFO] (main): Committed.2018-06-26 00:11:19.715 +0900 [INFO] (main): Next config diff: {\"in\":{},\"out\":{}}おわりになんとなく動くところまで持っていけたので記事にしました。Cloud Firestore も同じ要領で plugin 作れたりするのかなとぼんやり考えたりしました。","link":"https://syucream.hatenablog.jp/entry/2018/06/26/001404","isoDate":"2018-06-25T15:14:04.000Z","dateMiliSeconds":1529939644000,"authorName":"syucream","authorId":"syucream"},{"title":"protobuf の Marshal/Unmarshal や Any 型について確認していた","contentSnippet":"あるメッセージの送り元と送り先でスキーマを共有してそのスキーマに基づいたメッセージをやり取りしたいことが多々あると思います。そんな際にスキーマの記述を protobuf で行い、かつメッセージをシリアライズして送る構想について調査したメモを本稿で記載します。背景IoT だとか Microservices Architecture だとかウェブ・アプリサービスのユーザ行動分析だとか、AWS,GCP だとか何とか、世の中には多くのデータを生成するソースやデータ格納ストレージが生まれてきています。そんな中、どのようなスキーマ・フォーマットでデータを送信するかという点は未だ多くの開発者を悩ませているのではと考えます。これがビジネス上スケジュールの要件が厳しかったり、プロダクトの変化が大きかったり、データ送信ロジックを書く開発者が蓄積したデータを活用するシーンでサポートできるのであれば、スキーマレスデータベースにひとまずデータを格納しまくるのも良いかと思います。しかしながら個人的にはそうではない状況や、将来的な組織・人員の拡充を考えるとスキーマがあって欲しい気持ちが高まります。とはいえ、なるべくスキーマの変更には柔軟に対応できるようにあって欲しいものです。スキーマ定義をどうするかについて最初に思いついたのは JSON Schema なのですが、人が読み書きするのには適さないと感じます。その次に本稿で触れる、最近は gRPC メソッド定義などでもよく用いられる protobuf によるスキーマ定義を思いつきました。（実際にいくつかのサンプルに触れる前に、以下の記事に影響を受けました。 protobuf にこれから入門してみたい方は読んでみていただけると！）今さらProtocol Buffersと、手に馴染む道具の話確認したかったことprotobuf によるスキーマ定義自体は上記の記事より良さげだなと思っていたのですが、スキーマの更新の柔軟性、具体的には以下の点が気になっていました。メッセージのフィールドを増やした際の受信側の挙動タグの値が書き換わるような更新が起こった際の受信側の挙動Any による型情報付きメッセージの受信側の挙動これらを以下のシナリオのもと動作確認してみることにします。Twitter みたいなサービスのユーザの行動ログを protobuf で定義する最初は v1 として Event 型で行動ログのスキーマを定義するある程度時期が経過した際、ツイートの URI のような追加情報を足したくなったので RichEvent という新しい型でログを扱うことにした更に時期が経過した際、根本的に構造を見直したくなり、新しい v1 Event 型を定義してログを扱うことにしたログ送信コードを触る人と送信されたログを収集する人は異なり、システムも異なり、うまく連携できず受信側のスキーマ更新作業が遅れる場合がある確認結果NOTE: ここでは Go の protobuf packages を使ったりすること前提で書いています実験用コードを以下に記載します。github.com異なるバージョン間の Marshal/Unmarshalあまり綺麗にまとまっていませんが、以下のような形になりました。フィールドのタグと型が一致している部分に関してはバージョン更新が行われなかった場合でも最低限拾ってくれます。しかしながら protobuf は NON NULL 指定をして Unmarshal 時にエラーを起こすようなことができない（はず？なにか良い方法があればコメントください）ので各言語向けに protoc plugin が吐き出す getter などで自前でチェックする必要がありそうです。Check handling between messages might be compatible :v1 -> v1.5id:1 created_at:<seconds:1528128844 nanos:914007623 > event_type:TWEET user_id:1 value:\"test tweet!\"v1.5 -> v1id:1 created_at:<seconds:1528128844 nanos:914007623 > event_type:TWEET user_id:1 value:\"test tweet!\"Check handling between messages might be NOT compatible :v1 -> v2id:1 event_at:<seconds:1528128844 nanos:914007623 > event_type:TYPE_TWEETv2 -> v1id:1 created_at:<seconds:1528128844 nanos:914007623 > user_id:1Anyprotobuf の Any 型は message の型情報を付与してくれるので、 Marshal するメッセージをこれで wrap してみます。またメッセージを受け取る側では Unmarshal 前に型チェックを行うようにしてみます。型チェックは Go では ptypes package の ptypes.Is() で行えます。これに基づき、 v2 Event を Unmarshal する際の動作は以下のようになりました。v2 のメッセージを v1, v1.5 のメッセージとして Unmarshal しようとした際に ptypes.Is() のチェックが通らなくなっています。Check handling Any :type_url:\"type.googleapis.com/logging.v2.Event\" value:\"\\010\\001\\022\\014\\010\\314\\312\\325\\330\\005\\020\\307\\314\\352\\263\\003\\032\\014\\010\\314\\312\\325\\330\\005\\020\\307\\314\\352\\263\\003 \\001(\\0020\\0018\\002B\\013test tweet!\"Any is not loggingv1.EventAny is not loggingv1.RichEventid:1 event_at:<seconds:1528128844 nanos:914007623 > processed_at:<seconds:1528128844 nanos:914007623 > event_type:TYPE_TWEET event_source:SOURCE_PUBLIC_TIMELINE user_id:1 user_agent:UA_IOS value:\"test tweet!\"Any に wrap することによるバイナリサイズ増加実験に用いた v1 Event 型で言うと、そのメッセージを直接 Marshal した場合と Any で wrap した場合とでは以下のようなサイズ差がありました。当然ながら Any だと多少サイズが増えています。が、メッセージのフィールドが大きければ相対的に増加具合が目立たなくなるレベルになるかもしれません。Check marshaled size w/ Any :len(v1bin) : 33len(v1anyBin) : 73protobuf の周辺環境夜に出ている protoc plugin を使って、 protobuf によるスキーマ定義を周辺ツールと連携して行うなど強力な開発・データ分析環境が構築できるかと思われます。https://github.com/pseudomuto/protoc-gen-docproto ファイルからドキュメント生成https://github.com/GoogleCloudPlatform/protoc-gen-bq-schemaproto ファイルから BigQuery の schema を生成https://github.com/chrusty/protoc-gen-jsonschemaproto ファイルから JSON Schema を生成おわりにprotobuf によるスキーマ定義、なかなか良さそうに感じます。特に（メッセージサイズが増えることが許容できるなら） Any 型で包んでやり取りすると必要な際に簡単にメッセージ型チェックを行うことができ、便利だと思われます。また protoc plugin も多数存在し、単に protobuf を直接使う以外にも他のスキーマ定義を生成するシンプルな DSL として使うのもありかも知れません（JSON Schema とか）。この分野では Apache Avro と比較した検証ができていないので、どこかのタイミングでしてみたい気持ちもあります。","link":"https://syucream.hatenablog.jp/entry/2018/06/05/022810","isoDate":"2018-06-04T17:28:10.000Z","dateMiliSeconds":1528133290000,"authorName":"syucream","authorId":"syucream"},{"title":"MySQL のテーブル定義から外部キー制約を推測するツールを作った","contentSnippet":"GW 中の自分へのプチ課題として、表題の通りのツール hakagi(葉鍵, Leaf and Key) を作りました。ツール名には特に意味はありません。github.comテーブル名やカラム名から外部キー制約を張れるような気がするカラムを選出し、制約追加のための ALTER TABLE をするクエリを吐き出したりします。なぜ作ったのか外部キー制約は言うまでもなくデータの不整合を避けるのに有用ですが、いくつかの理由からこれを用いない選択肢も存在すると思われます。この辺りについては以下の記事なども参考にすると良さそうです。我々(主語が大きい)は何故MySQLで外部キーを使わないのかMySQLで外部キー制約を課すべきか - リジェクトされました外部キー制約を設けないことによるデメリットも幾つか生じると思っていて、その一つで自分が気になっている点として、「ER 図をツールで自動生成しにくい」があります。例えば MySQL Workbench は sql ファイルや稼働する MySQL データベースから ER 図を自動生成してくれる機能を持ちますが、エンティティ間のリレーションは外部キー制約を無くすと追えなくなります。この問題に対する緩和策として SchemaSpy を用いれば主キーをベースに外部キー制約を推測して ER 図生成を行ってくれます。しかしながらこの推測のルールはあまり小回りが効くものではありません。SchemaSpy のやっているような制約の推測を、図の自動生成とは分離してシンプルに扱い、かつルールを細かく追加したり ON/OFF したりしたい。そう考え今回紹介するツールの実装を開始しました。実装などワンバイナリでサクッと動かしたい希望があったため Go で書いています。MySQL データベースからテーブル定義や主キーの情報を集めるため information_schema データベースを参照し、推測を行って制約追加クエリを標準出力に出します。現在サポートする推測ルールは以下の通りです。あるカラム名と同じ名前の、別のテーブルのカラムが存在し、かつそれが id という名前ではなくそのテーブルで主キーである場合外部キー制約を追加するあるカラム名が <別のテーブル名の単数形>_id であり、かつ別のテーブルで id が主キーである場合外部キー制約を追加する標準出力に最終的に吐き出すフォーマットとしては今後は必要であれば PlantUML をサポートしたりしてもいいかも、と考えております。動作例以下のような、外部キー制約が全く設けられていないテーブル定義があったとします。CREATE TABLE IF NOT EXISTS users (  id    INT UNSIGNED NOT NULL AUTO_INCREMENT,  name  VARCHAR(255) NOT NULL,  age   INT UNSIGNED NOT NULL,  email VARCHAR(255) NOT NULL,  PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE IF NOT EXISTS contents (  id    INT UNSIGNED NOT NULL AUTO_INCREMENT,  title VARCHAR(255) NOT NULL,  text  TEXT NOT NULL,  user_id INT UNSIGNED NOT NULL,  PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE IF NOT EXISTS content_comments (  content_comment_id INT UNSIGNED NOT NULL AUTO_INCREMENT,  text               TEXT NOT NULL,  content_id         INT UNSIGNED NOT NULL,  user_id            INT UNSIGNED NOT NULL,  PRIMARY KEY(content_comment_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE IF NOT EXISTS content_comment_reactions (  content_comment_id INT UNSIGNED NOT NULL,  user_id            INT UNSIGNED NOT NULL,  reaction_id        INT UNSIGNED NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;CREATE TABLE IF NOT EXISTS reactions (  id        INT UNSIGNED NOT NULL AUTO_INCREMENT,  image_url VARCHAR(255) NOT NULL,  PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET=utf8;これをベースに MySQL Workbench で ER 図を生成すると以下のようになります。これに対し hakagi はいくつか外部キー制約を追加するクエリを吐き出してくれます。$ ./hakagi -dbuser root -targets hakagi_exampleALTER TABLE content_comment_reactions ADD CONSTRAINT FOREIGN KEY (content_comment_id) REFERENCES content_comments(content_comment_id);ALTER TABLE content_comment_reactions ADD CONSTRAINT FOREIGN KEY (user_id) REFERENCES users(id);ALTER TABLE content_comment_reactions ADD CONSTRAINT FOREIGN KEY (reaction_id) REFERENCES reactions(id);ALTER TABLE content_comments ADD CONSTRAINT FOREIGN KEY (content_id) REFERENCES contents(id);ALTER TABLE content_comments ADD CONSTRAINT FOREIGN KEY (user_id) REFERENCES users(id);ALTER TABLE contents ADD CONSTRAINT FOREIGN KEY (user_id) REFERENCES users(id);このクエリも用いることで、以下のような ER 図を生成することができました。おわりにまだ少ないサンプルに対して少ない推測ルールを適用しているだけなので、誤った推測結果が出たり推測が足りなかったりする場面があるかもしれません。このツールを用いる場面が数多くあれば、そこで出てきたユースケースを元にツールを機能拡張していきたいところです。","link":"https://syucream.hatenablog.jp/entry/2018/05/06/201257","isoDate":"2018-05-06T11:12:57.000Z","dateMiliSeconds":1525605177000,"authorName":"syucream","authorId":"syucream"},{"title":"技術書典4にサークル名「まいにちがきんようび。」で暗号通貨とDNSの本を配布します　","contentSnippet":"直前のお知らせになりますが、 技術書典4 に「まいにちがきんようび。」というサークル名で参加して技術系薄い本を配布します！techbookfest.org以下のような内容になっています！基礎から説明する暗号通貨 @lunatic_star今日から始める IOTA @syu_creamエンジニアのための DNS(再)入門(初級者編) @m_birdエンジニアのための DNS(再)入門(中級者編) @kdmn(表紙イラスト @fuga_romantica )配布部数は 50 程度になる予定です。また販売価格は一冊 500 円を予定しています。電子書籍での販売も予定しているため、もし売り切れるようなことがあれば（あるのかな...）そちらをご検討頂ければ幸いです。ぜひ当日、会場にてお会い出来れば嬉しいです！！！","link":"https://syucream.hatenablog.jp/entry/2018/04/18/214622","isoDate":"2018-04-18T12:46:22.000Z","dateMiliSeconds":1524055582000,"authorName":"syucream","authorId":"syucream"}]},"__N_SSG":true}